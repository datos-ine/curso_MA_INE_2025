[
  {
    "objectID": "unidad_3.html",
    "href": "unidad_3.html",
    "title": "Unidad 3: Meta-Análisis de Efectos Mixtos",
    "section": "",
    "text": "Este material es parte del curso interno de Introducción a la Revisión Sistemática con Meta-análisis  © 2025 Instituto Nacional de Epidemiología “Dr. Juan H. Jara” (ANLIS) -  CC BY-NC 4.0      \n\n\n Volver arriba"
  },
  {
    "objectID": "tipos_estimador.html",
    "href": "tipos_estimador.html",
    "title": "Estimadores de efecto",
    "section": "",
    "text": "Este material es parte del curso interno de Introducción a la Revisión Sistemática con Meta-análisis  © 2025 Instituto Nacional de Epidemiología “Dr. Juan H. Jara” (ANLIS) -  CC BY-NC 4.0"
  },
  {
    "objectID": "tipos_estimador.html#introducción",
    "href": "tipos_estimador.html#introducción",
    "title": "Estimadores de efecto",
    "section": "Introducción",
    "text": "Introducción\nEn esta sección repasaremos los principales estimadores de efecto utilizados en estudios epidemiológicos descriptivos y analíticos y presentaremos ejemplos para el ajuste de modelos de meta-análisis para cada caso usando R.\nComenzaremos por cargar el paquete meta:\n\nlibrary(meta)\n\nLoading required package: metadat\n\n\nLoading 'meta' package (version 8.0-2).\nType 'help(meta)' for a brief overview.\n\n\nPara facilitar la exploración de datos, utilizaremos el paquete tidyverse(Wickham et al. 2019):\n\nlibrary(tidyverse)\n\nEn la sección anterior, exploramos cómo personalizar los colores de los forest plot con los argumentos col.diamond y col.square. Para garantizar que los gráficos sean accesibles a personas con deficiencia en la percepción del color, utilizaremos paletas colorblind-friendly disponibles en el paquete scico (Pedersen y Crameri 2023):\n\n# Cargar paquete\nlibrary(scico)\n\n# Lista de paletas categóricas\nscico_palette_show(categorical = TRUE)\n\n\n\n\n\n\n\n\nEn los ejemplos siguientes, emplearemos la paleta \"buda\", que genera un gradiente de colores entre magenta y amarillo:\n\n# Paleta colorblind-friendly\npal &lt;- scico(n = 3, palette = \"buda\")"
  },
  {
    "objectID": "tipos_estimador.html#meta-análisis-en-estudios-descriptivos",
    "href": "tipos_estimador.html#meta-análisis-en-estudios-descriptivos",
    "title": "Estimadores de efecto",
    "section": "Meta-análisis en estudios descriptivos",
    "text": "Meta-análisis en estudios descriptivos\nEn los estudios descriptivos, los principales estimadores de efecto incluyen la correlación, la prevalencia y la tasa de incidencia. A continuación, presentaremos ejemplos prácticos de ajuste de modelos para cada uno de ellos.\n\nCorrelaciones\nLas correlaciones miden la fuerza y dirección de la relación entre dos variables numéricas continuas, calculándose como:\n\\[\nr_{xy} = \\frac{Cov_{xy}}{S_xS_y}\n\\]\ndonde:\n\n\\(Cov_{xy}\\) es la covarianza entre las variables X e Y.\n\\(S_x\\) y \\(S_y\\) son los desvíos estándar de cada variable.\n\nDado que los coeficientes de correlación solamente toman valores entre -1 y 1, su distribución no es simétrica, pudiendo afectar la estimación del error estándar en muestras pequeñas. Para corregir este sesgo y estabilizar la varianza, se utiliza la transformación z de Fisher.\nLa función metacor() ajusta modelos de meta-análisis para correlaciones y aplica automáticamente esta transformación mediante el argumento sm = \"ZCOR\".\nComo ejemplo, utilizaremos la base de datos dat.aloe2013, que recopila resultados de cinco estudios sobre la relación entre condiciones laborales y salud mental en trabajadores sociales que atienden infancias.\nComenzamos cargando los datos y explorando su estructura:\n\n# Cargar datos\ndatos_cor &lt;- dat.aloe2013\n\n# Explorar datos\nglimpse(datos_cor)\n\nRows: 5\nColumns: 5\n$ study &lt;chr&gt; \"Abu-Bader (2000)\", \"Cole et al. (2004)\", \"Wallach & Mueller (20…\n$ n     &lt;int&gt; 218, 232, 156, 382, 259\n$ tval  &lt;dbl&gt; 4.61, 6.19, 4.07, -0.77, 1.16\n$ preds &lt;int&gt; 4, 7, 6, 19, 15\n$ R2    &lt;dbl&gt; 0.240, 0.455, 0.500, 0.327, 0.117\n\n\nLas principales variables de interés son:\n\nR2: coeficiente de correlación.\nn: tamaño muestral en cada estudio.\nstudy: identificador del estudio.\n\nAjustamos el modelo de meta-análisis para correlaciones aplicando la transformación z de Fisher:\n\n# Ajuste del modelo\nmod_cor &lt;- metacor(\n  cor = R2,         # Coeficiente de correlación\n  n = n,            # Tamaño de la muestra\n  studlab = study,  # Identificador del estudio\n  data = datos_cor,     # Conjunto de datos\n  sm = \"ZCOR\",      # Transformación z de Fisher\n  common = TRUE,    # Modelo de efectos fijos\n  random = TRUE     # Modelo de efectos aleatorios\n)\n\n# Resumen del modelo ajustado\nmod_cor\n\nNumber of studies: k = 5\nNumber of observations: o = 1247\n\n                        COR           95%-CI     z  p-value\nCommon effect model  0.3195 [0.2685; 0.3687] 11.62 &lt; 0.0001\nRandom effects model 0.3320 [0.1903; 0.4602]  4.44 &lt; 0.0001\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.0259 [0.0064; 0.2528]; tau = 0.1608 [0.0800; 0.5028]\n I^2 = 84.9% [66.3%; 93.2%]; H = 2.57 [1.72; 3.84]\n\nTest of heterogeneity:\n     Q d.f.  p-value\n 26.44    4 &lt; 0.0001\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n- Fisher's z transformation of correlations\n\n\nLos resultados muestran una correlación positivasignificativa entre condiciones laborales y salud mental \\((p &lt; 0,001)\\). Sin embargo, la heterogeneidad es alta \\((I^2 = 84,9\\%)\\), lo que sugiere que existen diferencias importantes entre los estudios incluidos.\nGeneramos el forest plot para visualizar los resultados:\n\nforest(\n  mod_cor, \n  common = FALSE,               # Omite modelo de efectos fijos\n  col.diamond.random = pal[1],  # Magenta\n  col.square = pal[3]           # Amarillo\n)\n\n\n\n\n\n\n\n\nSi queremos personalizar los nombres de las etiquetas para que aparezcan en español, podemos modificar algunos de los siguientes argumentos:\n\nsmlab: etiqueta del estimador de efecto.\nleftlabs: etiquetas del panel izquierdo.\nrightlabs: etiquetas del panel derecho.\nhetlab: etiqueta para la heterogeneidad.\ntext.common: etiqueta del modelo de efectos fijos.\ntext.random: etiqueta del modelo de efectos aleatorios.\n\nAplicamos las modificaciones al gráfico anterior:\n\nforest(\n  mod_cor, \n  common = FALSE,                # Omite modelo de efectos fijos\n  col.diamond.random = pal[1],   # Magenta\n  col.square = pal[3],           # Amarillo\n  smlab = \"Correlación de Pearson\",\n  leftlabs = c(\"Estudio\", \"N\"),\n  rightlabs = c(\"COR\", \"95% IC\", \"Peso\"),\n  hetlab = \"Heterogeneidad: \",\n  text.random = \"Modelo de \\n efectos aleatorios\"\n)\n\n\n\n\n\n\n\n\n\n\nPrevalencia\nLa prevalencia representa la proporción de individuos con un evento de interés dentro de una población:\n\\[\np = \\frac{k}{n}\n\\]\ndonde:\n\n\\(k\\) es el número de individuos con la condición/evento.\n\\(n\\) es el tamaño total de la población o muestra.\n\nDado que las proporciones pueden estar cercanas a los valores extremos (0 o 1), su distribución es asimétrica, lo que afecta el cálculo del error estándar. Para corregir este problema, se aplica una transformación logit a los datos.\nLa función metaprop() permite ajustar modelos para prevalencias e incorpora automáticamente esta transformación con el argumento sm = \"PLOGIT\".\nPara ejemplificar, usaremos la base de datos dat.crisafulli2020, que contiene 26 estudios sobre la prevalencia de la distrofia muscular de Duchenne en recién nacidos:\n\n# Cargar datos\ndatos_prev &lt;- dat.crisafulli2020\n\n# Explorar datos\nglimpse(datos_prev)\n\nRows: 26\nColumns: 7\n$ study   &lt;chr&gt; \"Brooks (1977)\", \"Danieli (1977)\", \"Takeshita (1977)\", \"Drummo…\n$ pubyear &lt;int&gt; 1977, 1977, 1977, 1979, 1980, 1980, 1981, 1982, 1983, 1983, 19…\n$ country &lt;fct&gt; UK, IT, JP, NZ, AU, IT, IT, CA, FR, IT, DE, IT, JP, CA, NO, IT…\n$ from    &lt;int&gt; 1953, 1952, 1956, NA, 1960, 1952, 1955, 1950, 1978, 1969, 1977…\n$ to      &lt;int&gt; 1968, 1972, 1970, NA, 1971, 1972, 1974, 1979, 1978, 1980, 1984…\n$ cases   &lt;int&gt; 47, 66, 19, 2, 99, 105, 73, 110, 12, 156, 48, 76, 50, 5, 16, 2…\n$ total   &lt;int&gt; 177413, 234396, 91157, 10000, 532302, 371698, 301283, 420374, …\n\n\nLas principales variables de interés son:\n\ncasos: individuos con el evento.\ntotal: tamaño muestral.\nstudy: identificador de estudio .\n\nAjustamos el modelo de meta-análisis para proporciones aplicando la transformación logit:\n\n# Ajuste del modelo\nmod_prev &lt;- metaprop(\n  event = cases,     # Casos observados\n  n = total,         # Tamaño de la muestra\n  studlab = study,   # Identificador del estudio\n  data = datos_prev,      # Conjunto de datos\n  sm = \"PLOGIT\",     # Transformación logit\n  common = TRUE,     # Modelo de efectos fijos\n  random = TRUE,     # Modelo de efectos aleatorios\n  backtransf = TRUE, # Convertir resultados a proporciones\n  pscale = 100       # Expresar prevalencias como porcentaje\n)\n\n# Resumen del modelo ajustado\nmod_prev\n\nNumber of studies: k = 26\nNumber of observations: o = 6831388\nNumber of events: e = 1545\n\n                     events           95%-CI\nCommon effect model  0.0226 [0.0215; 0.0238]\nRandom effects model 0.0222 [0.0206; 0.0240]\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.0126; tau = 0.1121; I^2 = 33.2% [0.0%; 58.6%]; H = 1.22 [1.00; 1.55]\n\nTest of heterogeneity:\n          Q d.f. p-value\n Wald 37.41   25  0.0527\n LRT  39.01   25  0.0368\n\nDetails of meta-analysis methods:\n- Random intercept logistic regression model\n- Maximum-likelihood estimator for tau^2\n- Calculation of I^2 based on Q\n- Logit transformation\n- Events per 100 observations\n\n\nComo la prevalencia del evento es muy baja, vamos a expresarla en casos por 100 000 habitantes modificando el argumento pscale en el forest plot. Además, la heterogeneidad estadística es moderada \\((I^2 = 33,2\\%)\\), por lo que se puede omitir del gráfico el modelo de efectos fijos.\nGeneramos el forest plot para visualizar los resultados:\n\nforest(\n  mod_prev,\n  col.diamond = pal[1],   # Magenta\n  col.square = pal[3],    # Amarillo\n  common = FALSE,         # Omite modelo de efectos fijos\n  pscale = 100000,        # Escala a casos/100 000 habitantes\n  smlab = \"Prevalencia \\n (por 100 000 hab.)\",\n  leftlabs = c(\"Estudio\", \"Eventos\", \"N\"),\n  rightlabs = c(\"Eventos\", \"95% IC\"),\n  hetlab = \"Heterogeneidad: \",\n  text.random = \"Modelo de efectos aleatorios\"\n)\n\n\n\n\n\n\n\n\n\n\nTasa de incidencia\nLa tasa de incidencia o incidence rate (IR) se utiliza para eventos que ocurren a lo largo del tiempo y se define como:\n\\[\nIR = \\frac {k}{T}\n\\]​\ndonde:\n\n\\(k\\) es el número de eventos observados.\n\\(T\\) es la suma del tiempo-persona en riesgo en cada estudio.\n\nDado que las tasas de incidencia pueden ser pequeñas y asimétricas, se recomienda aplicar una transformación logarítmica a los datos para estabilizar su varianza.\nLa función metarate() ajusta modelos de meta-análisis para tasas de incidencia, aplicando esta transformación (sm = \"IRLN\").\nComo ejemplo, usamos la base dat.nielweise2008, que contiene 9 estudios sobre la incidencia de infecciones sanguíneas asociadas al uso de catéteres:\n\n# Cargar datos\ndatos_inc &lt;- dat.nielweise2008\n\n# Explorar datos\nglimpse(datos_inc)\n\nRows: 9\nColumns: 7\n$ study   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9\n$ authors &lt;chr&gt; \"Bong et al.\", \"Ciresi et al.\", \"Hanna et al.\", \"Harter et al.…\n$ year    &lt;int&gt; 2003, 1996, 2004, 2002, 2001, 2005, 1997, 2005, 1996\n$ x1i     &lt;int&gt; 7, 8, 3, 6, 1, 1, 17, 3, 2\n$ t1i     &lt;int&gt; 1344, 1600, 12012, 1536, 370, 729, 6760, 1107, 320\n$ x2i     &lt;int&gt; 11, 8, 14, 10, 1, 8, 15, 7, 3\n$ t2i     &lt;int&gt; 1988, 1461, 10962, 1503, 483, 913, 6840, 1015, 440\n\n\nLas principales variables de interés son:\n\nx2i: casos observados.\nt2i: años-persona.\nauthors: identificador de estudio.\n\nAjustamos el modelo de meta-análisis para tasas de incidencia aplicando la transformación logarítmica:\n\n# Ajuste del modelo\nmod_inc &lt;- metarate(\n  event = x2i,            # Casos observados\n  time = t2i,             # Tiempo-persona en riesgo\n  studlab = authors,      # Identificador del estudio\n  data = datos_inc,       # Conjunto de datos\n  sm = \"IRLN\",            # Transformación logarítmica\n  common = TRUE,          # Modelo de efectos fijos\n  random = TRUE,          # Modelo de efectos aleatorios\n)\n\n# Resumen del modelo ajustado\nmod_inc\n\nNumber of studies: k = 9\nNumber of events: e = 77\n\n                       rate           95%-CI\nCommon effect model  0.0039 [0.0031; 0.0048]\nRandom effects model 0.0043 [0.0027; 0.0070]\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.3699 [0.0940; 1.5145]; tau = 0.6082 [0.3065; 1.2307]\n I^2 = 78.0% [58.4%; 88.4%]; H = 2.13 [1.55; 2.93]\n\nTest of heterogeneity:\n     Q d.f.  p-value\n 36.38    8 &lt; 0.0001\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n- Log transformation\n\n\nComo la tasa de incidencia del evento es muy baja, vamos a expresarla en casos por 1000 años-persona modificando el argumento pscale en el forest plot. Además, la heterogeneidad estadística es alta \\((I^2 = 78\\%)\\), por lo que se puede omitir del gráfico el modelo de efectos fijos.\nGeneramos el forest plot para visualizar los resultados:\n\nforest(\n  mod_inc,\n  col.diamond = pal[1],  # Magenta\n  col.square = pal[3],   # Amarillo\n  common = FALSE,        # Omite modelo de efectos fijos\n  pscale = 1000,         # Escala a casos/1000 años-persona\n  smlab = \"Tasa de incidencia \\n (1000 años-persona)\",\n  leftlabs = c(\"Estudio\", \"Eventos\", \"Tiempo\"),\n  rightlabs = c(\"Eventos\", \"95% IC\", \"Peso\"),\n  hetlab = \"Heterogeneidad: \",\n  text.random = \"Modelo de efectos aleatorios\"\n)"
  },
  {
    "objectID": "tipos_estimador.html#meta-análisis-en-estudios-analíticos",
    "href": "tipos_estimador.html#meta-análisis-en-estudios-analíticos",
    "title": "Estimadores de efecto",
    "section": "Meta-análisis en estudios analíticos",
    "text": "Meta-análisis en estudios analíticos\nDentro de los estudios analíticos (observacionales y/o experimentales), los estimadores de efecto más comunes son la diferencia de medias, el odds-ratio (OR), el riesgo relativo (RR) y la razón de tasas de incidencia (IRR). A continuación, presentamos ejemplos prácticos de ajuste de modelos para cada uno de ellos.\n\nDiferencia de medias\nLa diferencia de medias entre dos grupos de exposición se define como:\n\\[\nMD = \\bar{x_e} - \\bar{x_c}\n\\]\ndonde:\n\n\\(\\bar{x_e}\\) es la media muestral del grupo expuesto o tratado.\n\\(\\bar{x_c}\\) es la media muestral del grupo no expuesto o control.\n\nEl cálculo de la diferencia de medias requiere que todas las mediciones se hayan tomado en la misma escala. Para los modelos de meta-análisis, se utiliza la diferencia de medias estandarizada, que elimina la dependencia de las unidades de medición al ponderar por el desvío estándar.\nLa función metacont() ajusta modelos de meta-análisis para diferencias de medias estandarizadas con el argumento sm = \"SMD\".\nComo ejemplo, utilizaremos la base de datos dat.furukawa2003, que contiene resultados de 17 estudios sobre la efectividad de la dosis de antidepresivos tricíclicos en casos de depresión severa.\nComenzamos cargando los datos y explorando su estructura:\n\n# Cargar datos\ndatos_md &lt;- dat.furukawa2003\n\n# Explorar datos\nglimpse(datos_md)\n\nRows: 17\nColumns: 7\n$ author &lt;chr&gt; \"Blashki(75&150)\", \"Hormazabal(86)\", \"Jacobson(75-100)\", \"Jenki…\n$ Ne     &lt;int&gt; 13, 17, 10, 7, 73, 26, 17, 11, 105, 22, 13, 29, 13, 78, 23, 11,…\n$ Me     &lt;dbl&gt; 6.40, 11.00, 17.50, 12.30, 15.70, 8.50, 25.50, 6.20, -8.10, 13.…\n$ Se     &lt;dbl&gt; 5.40, 8.20, 8.80, 9.90, 10.60, 11.00, 24.00, 7.60, 3.90, 2.30, …\n$ Nc     &lt;int&gt; 18, 16, 6, 7, 73, 28, 10, 10, 46, 19, 15, 39, 13, 71, 23, 11, 18\n$ Mc     &lt;dbl&gt; 11.40, 19.00, 23.00, 20.00, 18.70, 14.50, 53.20, 10.00, -8.50, …\n$ Sc     &lt;dbl&gt; 9.60, 8.20, 8.80, 10.50, 10.60, 11.00, 11.20, 7.60, 5.20, 1.30,…\n\n\nLas principales variables de interés son:\n\nMe: media muestral en grupo expuesto/tratamiento.\nSe: desvío estándar de la media en grupo expuesto/tratamiento.\nNe: tamaño muestral en grupo expuesto/tratamiento.\nMc: media muestral en grupo no expuesto/control.\nSc: desvío estándar de la media en grupo no expuesto/control.\nNc: tamaño muestral en grupo no expuesto/control.\nauthor: identificador de estudio.\n\nAjustamos el modelo de meta-análisis para diferencia de medias estandarizada:\n\n# Ajuste del modelo\nmod_md &lt;- metacont(\n  n.e = Ne,         # Tamaño muestral grupo expuesto\n  mean.e = Me,      # Media en grupo expuesto\n  sd.e = Se,        # Desvío estándar en grupo expuesto\n  n.c = Nc,         # Tamaño muestral grupo control\n  mean.c = Mc,      # Media en grupo control\n  sd.c = Sc,        # Desvío estándar en grupo control\n  studlab = author, # Identificador del estudio\n  data = datos_md,  # Conjunto de datos\n  sm = \"SMD\",       # Diferencia de medias estandarizada\n  common = TRUE,    # Modelo de efectos fijos\n  random = TRUE,    # Modelo de efectos aleatorios\n)\n\n# Resumen del modelo ajustado \nmod_md\n\nNumber of studies: k = 17\nNumber of observations: o = 902 (o.e = 479, o.c = 423)\n\n                         SMD             95%-CI     z  p-value\nCommon effect model  -0.3918 [-0.5286; -0.2551] -5.62 &lt; 0.0001\nRandom effects model -0.6056 [-0.9326; -0.2787] -3.63   0.0003\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.3415 [0.1422; 1.1791]; tau = 0.5844 [0.3771; 1.0859]\n I^2 = 72.6% [55.5%; 83.1%]; H = 1.91 [1.50; 2.43]\n\nTest of heterogeneity:\n     Q d.f.  p-value\n 58.38   16 &lt; 0.0001\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n- Hedges' g (bias corrected standardised mean difference; using exact formulae)\n\n\nLa diferencia de medias estandarizada entre el grupo tratado y el grupo control es estadísticamente significativa \\((p&lt;0,001)\\) y presenta alta heterogeneidad \\((I^2 = 73,8\\%)\\), por lo que puede descartarse el modelo de efectos fijos.\nGeneramos el forest plot para visualizar los resultados:\n\nforest(\n  mod_md,\n  common = FALSE,        # Omite modelo de efectos fijos\n  col.diamond = pal[1],  # Magenta\n  col.square = pal[3],   # Amarillo claro\n  smlab = \"Diferencia de medias \\n estandarizada\",\n  leftlabs = c(\"Estudio\", \n               rep(c(\"Total\", \"Media\", \"SD\"),2)),\n  rightlabs = c(\"SMD\", \"95% IC\", \"Peso\"),\n  hetlab = \"Heterogeneidad: \",\n  text.random = \"Modelo de efectos aleatorios\"\n  )\n\n\n\n\n\n\n\n\nPara mejorar la visualización, podríamos omitir algunas columnas del panel izquierdo usando el argumento leftcols:\n\nforest(\n  mod_md,\n  common = FALSE,        # Omite modelo de efectos fijos\n  col.diamond = pal[1],  # Magenta\n  col.square = pal[3],   # Amarillo claro\n  leftcols = \"studlab\",  # Controla columnas panel izquierdo\n  smlab = \"Diferencia de medias \\n estandarizada\",\n  leftlabs = \"Estudio\",\n  rightlabs = c(\"SMD\", \"95% IC\", \"Peso\"),\n  hetlab = \"Heterogeneidad: \",\n  text.random = \"Modelo de efectos aleatorios\"\n  )\n\n\n\n\n\n\n\n\n\n\nOdds-ratio\nEl odds ratio (OR) o razón de productos cruzados se define como el cociente entre los odds del evento en el grupo expuesto/tratamiento y en el grupo no expuesto/control:\n\\[\nOR =  \\frac{a/b}{c/d}\n\\]\ndonde:\n\n\\(a\\) es el número de eventos en el grupo expuesto/tratamiento.\n\\(b\\) es el número de individuos sin el evento en el grupo expuesto/tratamiento.\n\\(c\\) es el número de eventos en el grupo no expuesto/control.\n\\(d\\) es el número de individuos sin el evento en el grupo no expuesto/control.\n\nEl OR solo puede tomar valores positivos \\((0-\\infty)\\), donde:\n\n\\(OR = 1\\) indica ausencia de efecto.\n\\(OR &gt;1\\) sugiere un aumento en la probabilidad de ocurrencia del evento en el grupo expuesto.\n\\(OR &lt; 1\\) sugiere un posible efecto protector de la exposición o tratamiento.\n\nDado que el OR sigue una distribución asimétrica, su análisis estadístico puede ser complejo. Para estabilizar la varianza y aproximar una distribución normal, se aplica una transformación logarítmica.\nLa función metabin() ajusta modelos de meta-análisis para OR e incorpora automáticamente esta transformación mediante el argumento sm = \"OR\". Además, incluye una corrección de continuidad para manejar estudios con valores de eventos iguales a cero.\nEl siguiente ejemplo utiliza la base de datos dat.collins1985b, que contiene información de 9 estudios sobre el efecto de los diuréticos en la prevención de preeclampsia.\nComenzamos cargando los datos y explorando su estructura:\n\n# Carga datos\ndatos_or &lt;- dat.collins1985b\n\n# Explorar datos\nglimpse(datos_or)\n\nRows: 9\nColumns: 16\n$ id      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9\n$ author  &lt;chr&gt; \"Weseley & Douglas\", \"Flowers et al.\", \"Menzies\", \"Fallis et a…\n$ year    &lt;int&gt; 1962, 1962, 1964, 1964, 1964, 1965, 1966, 1971, 1975\n$ pre.nti &lt;int&gt; 131, 385, 57, 38, 1011, 1370, 506, 108, 153\n$ pre.nci &lt;int&gt; 136, 134, 48, 40, 760, 1336, 524, 103, 102\n$ pre.xti &lt;int&gt; 14, 21, 14, 6, 12, 138, 15, 6, 65\n$ pre.xci &lt;int&gt; 14, 17, 24, 18, 35, 175, 20, 2, 40\n$ oedema  &lt;int&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0\n$ fup.nti &lt;int&gt; 131, 335, 57, 34, 1011, 1370, 506, 108, 153\n$ fup.nci &lt;int&gt; 136, 110, 48, 40, 760, 1336, 524, 103, 102\n$ ped.xti &lt;int&gt; 1, 6, 3, 1, 14, 24, 14, 0, 0\n$ ped.xci &lt;int&gt; 4, 3, 2, 3, 13, 19, 16, 0, 0\n$ stb.xti &lt;int&gt; 1, 3, 1, 0, 6, NA, 6, 0, 0\n$ stb.xci &lt;int&gt; 2, 2, 1, 1, 5, NA, 9, 0, 0\n$ ned.xti &lt;int&gt; 0, 3, 2, 1, 8, NA, 8, 0, 0\n$ ned.xci &lt;int&gt; 2, 1, 1, 2, 8, NA, 7, 0, 0\n\n\nLas principales variables de interés son:\n\npre.xti: número de eventos en el grupo expuesto/tratamiento.\npre.nti: tamaño muestral en el grupo expuesto/tratamiento.\npre.xci: número de eventos en el grupo no expuesto/control.\npre.nti: tamaño muestral en el grupo no expuesto/control.\nauthor: identificador del estudio.\n\n\n# Ajusta modelo\nmod_or &lt;- metabin(\n  event.e = pre.xti,  # Eventos en el grupo expuesto/tratamiento\n  n.e = pre.nti,      # Tamaño muestral en el grupo expuesto/tratamiento\n  event.c = pre.xci,  # Eventos en el grupo no expuesto/control\n  n.c = pre.nci,      # Tamaño muestral en el grupo control\n  studlab = author,   # Identificador único de cada estudio\n  data = datos_or,       # Conjunto de datos\n  sm = \"OR\",          # Odds-ratio\n  common = TRUE,      # Modelo de efectos fijos\n  random = TRUE       # Modelo de efectos aleatorios\n)\n\n# Resumen del modelo ajustado\nmod_or\n\nNumber of studies: k = 9\nNumber of observations: o = 6942 (o.e = 3759, o.c = 3183)\nNumber of events: e = 636\n\n                         OR           95%-CI     z  p-value\nCommon effect model  0.6677 [0.5620; 0.7932] -4.60 &lt; 0.0001\nRandom effects model 0.5956 [0.3843; 0.9233] -2.32   0.0205\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.3008 [0.0723; 2.2027]; tau = 0.5484 [0.2689; 1.4842]\n I^2 = 70.7% [41.8%; 85.2%]; H = 1.85 [1.31; 2.60]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 27.26    8  0.0006\n\nDetails of meta-analysis methods:\n- Mantel-Haenszel method (common effect model)\n- Inverse variance method (random effects model)\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n\n\nEl OR combinado sugiere que el uso de diuréticos reduce las probabilidades de preeclampsia en comparación con el grupo control \\((p = 0,021)\\). La heterogeneidad estadística es alta \\((I^2 = 70,7\\%)\\). Debido a esta heterogeneidad, el modelo de efectos aleatorios es el más apropiado.\nGeneramos el forest plot para visualizar los resultados:\n\nforest(\n  mod_or,\n  common = FALSE,        # Omite modelo de efectos fijos\n  col.diamond = pal[1],  # Magenta\n  col.square = pal[3],   # Amarillo claro\n  leftcols = \"studlab\",  # Columnas panel izquierdo\n  smlab = \"Odds-ratio\",\n  leftlabs = \"Estudio\",\n  rightlabs = c(\"OR\", \"95% IC\", \"Peso\"),\n  hetlab = \"Heterogeneidad: \",\n  text.random = \"Modelo de efectos aleatorios\"\n  )\n\n\n\n\n\n\n\n\n\n\nRiesgo relativo\nEl riesgo relativo (RR) o risk ratio mide la razón entre las probabilidades de desarrollar un evento en el grupo expuesto y en el grupo control:\n\\[\nRR =  \\frac{a/(a + b)}{c/(c + d)}\n\\]\ndonde:\n\n\\(a\\) es el número de eventos en el grupo expuesto/tratamiento.\n\\(b\\) es el número de individuos sin el evento en el grupo expuesto/tratamiento.\n\\(c\\) es el número de eventos en el grupo no expuesto/control.\n\\(d\\) es el número de individuos sin el evento en el grupo no expuesto/control.\n\nAl igual que el OR, el RR es una medida asimétrica y solo toma valores positivos \\((0-\\infty)\\). Para estabilizar la varianza y mejorar la interpretación estadística, se usa una transformación logarítmica, lo que permite modelar el RR en un intervalo simétrico y facilita la comparación entre estudios.\nLa función metabin() permite calcular el RR mediante el argumento sm = \"RR\", que aplica automáticamente la transformación logarítmica. Debido a la similitud en el cálculo con el OR, omitiremos el ejemplo para esta medida de asociación.\n\n\nRazón de tasas de incidencia\nLa razón de tasas de incidencia (incidence rate ratio, IRR) compara la frecuencia de eventos en dos grupos considerando el tiempo-persona de exposición:\n\\[\nIRR = \\frac{IR_e}{IR_c}\n\\]\ndonde:\n\n\\(IR_e\\) es la tasa de incidencia en el grupo expuesto/tratamiento.\n\\(IR_c\\) es la tasa de incidencia en el grupo no expuesto/control.\n\nAl igual que para la tasa de incidencia, se recomienda realizar la transformación logarítmica de los datos para aproximarlos a una distribución normal.\nEn meta, la función metainc() ajusta modelos de IRR con sm = \"IRR\", aplicando automáticamente la transformación logarítmica.\nA modo de ejemplo, volveremos a usar la base datos_inc, esta vez comparando entre grupo de exposición y control.\nLas principales variables de interés para este caso son:\n\nx1i: casos observados en grupo expuesto/tratamiento.\nt1i: años-persona en grupo expuesto/tratamiento.\nx2i: casos observados en grupo no expuesto/control.\nt2i: años-persona en grupo no expuesto/control.\nauthors: identificador de estudio.\n\nAjustamos el modelo de meta-análisis para IRR aplicando la transformación logarítmica:\n\n# Ajusta modelo\nmod_irr &lt;- metainc(\n  event.e = x1i,          # Casos en grupo expuesto\n  time.e = t1i,           # Tiempo-persona en grupo expuesto\n  event.c = x2i,          # Casos en grupo control\n  time.c = t2i,           # Tiempo-persona en grupo control\n  studlab = authors,      # Identificador único de cada estudio\n  data = datos_inc,       # Conjunto de datos\n  sm = \"IRR\",             # Razón de tasas de incidencia\n  common = TRUE,          # Modelo de efectos fijos\n  random = TRUE,          # Modelo de efectos aleatorios\n  )\n\n# Resumen del modelo ajustado\nmod_irr\n\nNumber of studies: k = 9\nNumber of events: e = 125\n\n                        IRR           95%-CI     z p-value\nCommon effect model  0.6602 [0.4608; 0.9459] -2.26  0.0236\nRandom effects model 0.6728 [0.4314; 1.0494] -1.75  0.0806\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.0936 [0.0000; 1.4845]; tau = 0.3060 [0.0000; 1.2184]\n I^2 = 17.5% [0.0%; 59.5%]; H = 1.10 [1.00; 1.57]\n\nTest of heterogeneity:\n    Q d.f. p-value\n 9.70    8  0.2869\n\nDetails of meta-analysis methods:\n- Mantel-Haenszel method (common effect model)\n- Inverse variance method (random effects model)\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n\n\nLos resultados del modelo muestran que existe una disminución en el riesgo del evento en el grupo tratado que es significativa en el modelo de efectos fijos \\((p = 0,24)\\), pero no en el de efectos aleatorios \\((p = 0,81)\\), con baja heterogeneidad estadística \\((I^2 = 17,5\\%)\\).\nGeneramos el forest plot para visualizar los resultados, usando los argumentos col.diamond.random y col.diamond.common para mostrar en diferentes colores los estimadores globales del modelo de efectos fijos y el modelo de efectos aleatorios:\n\nforest(\n  mod_irr,\n  col.diamond.random = pal[1],  # Magenta\n  col.diamond.common = pal[2],   # Rosa\n  col.square = pal[3],          # Amarillo claro\n  leftcols = \"studlab\",         # Columnas panel izquierdo\n  smlab = \"Razón de tasas de incidencia\",\n  leftlabs = \"Estudio\",\n  rightlabs = c(\"IRR\", \"95% IC\", \"Peso (fijo)\", \"Peso (aleatorio)\"),\n  hetlab = \"Heterogeneidad: \",\n  text.random = \"Modelo de efectos aleatorios\",\n  text.common = \"Modelo de efectos fijos\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\n\nDebido a la extensión del curso, nos enfocaremos exclusivamente en la implementación en R de los modelos para cada medida de asociación. Quienes deseen profundizar en el desarrollo matemático de estos modelos pueden consultar los capítulos 3 y 4.2 de Harrer et al. (2021).\nActualmente, el paquete meta no incluye funciones específicas para modelar el tiempo hasta el evento (hazard ratio, HR). Sin embargo, si los estudios reportan el log-HR y su error estándar, es posible utilizar la función metagen() con el argumento sm = \"HR\" para obtener una estimación combinada del efecto. Para una explicación detallada del proceso, pueden consultar el capítulo 2.6.1 de Schwarzer, Carpenter, y Rücker (2015)."
  },
  {
    "objectID": "mod_bias.html",
    "href": "mod_bias.html",
    "title": "Exploración de la heterogeneidad",
    "section": "",
    "text": "Este material es parte del curso interno de Introducción a la Revisión Sistemática con Meta-análisis  © 2025 Instituto Nacional de Epidemiología “Dr. Juan H. Jara” (ANLIS) -  CC BY-NC 4.0"
  },
  {
    "objectID": "mod_bias.html#análisis-de-moderadores",
    "href": "mod_bias.html#análisis-de-moderadores",
    "title": "Exploración de la heterogeneidad",
    "section": "Análisis de moderadores",
    "text": "Análisis de moderadores\nEl análisis de moderadores permite explorar fuentes de heterogeneidad entre los estudios incluidos en un meta-análisis. Se puede realizar mediante la inclusión de una variable independiente categórica (análisis de subgrupos) o numérica (metarregresión).\nEste procedimiento contribuye a evaluar hipótesis sobre variaciones en la magnitud del efecto entre estudios y a interpretar diferencias observadas en los resultados. Sin embargo, para evitar sesgos de selección, las variables utilizadas como moderadores deben definirse de antemano, durante la extracción de datos relevantes para la revisión sistemática.\nEl análisis de moderadores consta de dos etapas:\n\nEstimación del efecto dentro de cada subgrupo.\nPrueba estadística para evaluar diferencias entre subgrupos.\n\n\nAnálisis de sugbgrupos\nPara evaluar el efecto de una variable categórica, se puede utilizar el argumento subgroup al ajustar un modelo de meta-análisis. A continuación, realizaremos un análisis de subgrupos sobre el modelo de prevalencia ajustado previamente, considerando el país (country) como moderador.\nComenzaremos cargando los paquetes necesarios:\n\n# Carga de paquetes\nlibrary(meta)       # Modelos de meta-análisis\nlibrary(scico)      # Paletas de colores accesibles\nlibrary(janitor)    # Tablas de frecuencia\nlibrary(tidyverse)  # Manejo de datos\n\n# Paleta colorblind-friendly\npal &lt;- scico(n = 4, palette = \"buda\")\n\nCargamos los datos y exploramos su estructura:\n\n# Cargar datos\ndatos_prev &lt;- dat.crisafulli2020\n\n# Inspeccionar estructura de los datos\nglimpse(datos_prev)\n\nRows: 26\nColumns: 7\n$ study   &lt;chr&gt; \"Brooks (1977)\", \"Danieli (1977)\", \"Takeshita (1977)\", \"Drummo…\n$ pubyear &lt;int&gt; 1977, 1977, 1977, 1979, 1980, 1980, 1981, 1982, 1983, 1983, 19…\n$ country &lt;fct&gt; UK, IT, JP, NZ, AU, IT, IT, CA, FR, IT, DE, IT, JP, CA, NO, IT…\n$ from    &lt;int&gt; 1953, 1952, 1956, NA, 1960, 1952, 1955, 1950, 1978, 1969, 1977…\n$ to      &lt;int&gt; 1968, 1972, 1970, NA, 1971, 1972, 1974, 1979, 1978, 1980, 1984…\n$ cases   &lt;int&gt; 47, 66, 19, 2, 99, 105, 73, 110, 12, 156, 48, 76, 50, 5, 16, 2…\n$ total   &lt;int&gt; 177413, 234396, 91157, 10000, 532302, 371698, 301283, 420374, …\n\n\nUsaremos la función tabyl() del paquete janitor (Firke 2024) para generar una tabla de frecuencias de los niveles de la variable country:\n\ntabyl(datos_prev, country) |&gt;   # Genera tabla de frecuencia\n  arrange(-n) |&gt;                # Ordena por frecuencia\n  adorn_pct_formatting()        # Proporciones a porcentajes\n\n country n percent\n      IT 6   23.1%\n      CA 3   11.5%\n      UK 3   11.5%\n      JP 2    7.7%\n      AU 1    3.8%\n      BE 1    3.8%\n      CY 1    3.8%\n      DE 1    3.8%\n      DK 1    3.8%\n      EE 1    3.8%\n      FR 1    3.8%\n      NL 1    3.8%\n      NO 1    3.8%\n      NZ 1    3.8%\n      SI 1    3.8%\n      US 1    3.8%\n\n\nDado que la mayoría de los estudios provienen de Italia y la frecuencia en otros países es baja, creamos una variable dicotómica pais_cat:\n\ndatos_prev &lt;- datos_prev |&gt; \n  mutate(pais_cat = if_else(\n    country  == \"IT\", # Condición\n    \"Italia\",         # Valor si la condición se cumple\n    \"Otro/s\" )        # Valor si la condición no se cumple\n  )\n\nAjustamos el modelo de meta-análisis para proporciones, incorporando la variable pais_cat como moderador:\n\n# Ajuste modelo\nmod_sg &lt;- metaprop(\n  event = cases,          # Casos observados\n  n = total,              # Tamaño de la muestra\n  studlab = study,        # Identificador del estudio\n  data = datos_prev,      # Conjunto de datos\n  sm = \"PLOGIT\",          # Transformación logit\n  common = FALSE,         # Omitir modelo de efectos fijos\n  random = TRUE,          # Modelo de efectos aleatorios\n  pscale = 100000,        # Escala a casos/100 000 habitantes\n  subgroup = pais_cat     # Moderador categórico\n)\n\n# Resumen del modelo ajustado\nmod_sg\n\nNumber of studies: k = 26\nNumber of observations: o = 6831388\nNumber of events: e = 1545\n\n                      events             95%-CI\nRandom effects model 22.2342 [20.6056; 23.9915]\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.0126; tau = 0.1121; I^2 = 33.2% [0.0%; 58.6%]; H = 1.22 [1.00; 1.55]\n\nTest of heterogeneity:\n          Q d.f. p-value\n Wald 37.41   25  0.0527\n LRT  39.01   25  0.0368\n\nResults for subgroups (random effects model):\n                    k  events             95%-CI  tau^2    tau     Q   I^2\npais_cat = Otro/s  20 20.9400 [19.0685; 22.9951] 0.0122 0.1104 26.02 27.0%\npais_cat = Italia   6 25.0380 [22.5505; 27.7999] 0.0033 0.0577  5.97 16.2%\n\nTest for subgroup differences (random effects model):\n                  Q d.f. p-value\nBetween groups 6.22    1  0.0126\n\nDetails of meta-analysis methods:\n- Random intercept logistic regression model\n- Maximum-likelihood estimator for tau^2\n- Calculation of I^2 based on Q\n- Logit transformation\n- Events per 100000 observations\n\n\nLa salida del modelo incluye dos secciones adicionales:\n\nResults for subgroups (random effects model): muestra los resultados para cada categoría del moderador, incluyendo:\n\nIdentificador de las categorías del moderador.\nk: número de estudios en la categoría/subgrupo.\nevents: prevalencia en la categoría/subgrupo.\n95%-CI: intervalo de confianza al 95% para la prevalencia en la categoría/subgrupo.\ntau^2: varianza dentro de la categoría/subgrupo.\ntau: desvío estándar en la categoría/subgrupo.\nQ: estadístico Q de Cochran para la categoría/subgrupo.\nI^2: porcentaje de heterogeneidad observado en la categoría/subgrupo.\n\nTest for subgroup differences (random effects model): prueba de hipótesis para detectar diferencias significativas entre subgrupos.\n\nEn este ejemplo, observamos una prevalencia significativamente mayor en Italia en comparación con otros países \\((p = 0,013)\\).\nPodemos visualizar los resultados del análisis de moderadores incorporando los siguiente argumentos a la función forest():\n\nlayout = \"subgroup\": muestra los estimadores de efecto para cada subgrupo y el estimador global, omitiendo los resultados de los estudios individuales.\nsort.subgroup: ordena alfabeticamente las categorías de la variable moderadora.\ncalcwidth.subgroup: ajusta el ancho del forest plot para que se muestren correctamente las etiquetas de las categorías/subgrupos.\ncalcwidth.tests: ajusta el ancho del forest plot para que se muestren correctamente las etiquetas del test de hipótesis de diferencias en las categorías/subgrupos.\nprint.subgroup.name: muestra la etiqueta de la variable moderadora delante de cada categoría (TRUE, por defecto) o lo oculta (FALSE).\nlabel.test.subgroup.common: etiqueta para los resultados del test para diferencias entre subgrupos en el modelo de efectos fijos.\nlabel.test.subgroup.random: etiqueta para los resultados del test para diferencias entre subgrupos en el modelo de efectos aleatorios.\n\nEn nuestro ejemplo:\n\nforest(\n  mod_sg,\n  layout = \"subgroup\",\n  sort.subgroup = TRUE,\n  calcwidth.subgroup = TRUE,\n  calcwidth.tests = TRUE,\n  print.subgroup.name = FALSE,\n  smlab = \"Prevalencia \\n (por 100 000 hab.)\",\n  rightlabs = c(\"Eventos\", \"95% IC\"),\n  hetlab = \"Heterogeneidad: \",\n  text.random = \"Modelo de efectos aleatorios\", \n  label.test.subgroup.random = \"Diferencias entre subgrupos\"\n)\n\n\n\n\n\n\n\n\nA diferencia de modelos sin moderadores, col.diamond modifica el color del estimador global y de los estimadores para cada subgrupo. En cambio, col.square no tiene efecto sobre los colores del forest plot.\n\n\nMetarregresión\nPara evaluar el efecto de una variable continua, como el año de publicación (pubyear), primero ajustamos el modelo de meta-análisis:\n\n# Ajuste modelo\nmod_prev &lt;- metaprop(\n  event = cases,          # Casos observados\n  n = total,              # Tamaño de la muestra\n  studlab = study,        # Identificador del estudio\n  data = datos_prev,      # Conjunto de datos\n  sm = \"PLOGIT\",          # Transformación logit\n  common = FALSE,         # Omitir modelo de efectos fijos\n  random = TRUE,          # Modelo de efectos aleatorios\n  pscale = 100000,        # Escala a casos/100 000 habitantes\n )\n\nAl modelo anterior le aplicamos la función metareg(), incluyendo pubyear como moderador en el argumento formula:\n\n# Ajustar modelo de metarregresión\nmod_year &lt;- metareg(mod_prev,\n                    formula = ~ pubyear)\n\n# Resumen del modelo ajustado\nsummary(mod_year)\n\n\nMixed-Effects Model (k = 26; tau^2 estimator: ML)\n\n  logLik  deviance       AIC       BIC      AICc   \n-75.9315   14.5514  157.8629  161.6372  158.9538   \n\ntau^2 (estimated amount of residual heterogeneity):     0.0067\ntau (square root of estimated tau^2 value):             0.0816\nI^2 (residual heterogeneity / unaccounted variability): 26.61%\nH^2 (unaccounted variability / sampling variability):   1.36\n\nTests for Residual Heterogeneity:\nWld(df = 24) = 28.8391, p-val = 0.2262\nLRT(df = 24) = 30.5162, p-val = 0.1682\n\nTest of Moderators (coefficient 2):\nQM(df = 1) = 7.1208, p-val = 0.0076\n\nModel Results:\n\n         estimate      se     zval    pval    ci.lb    ci.ub     \nintrcpt    8.5560  6.3550   1.3463  0.1782  -3.8996  21.0116     \npubyear   -0.0085  0.0032  -2.6685  0.0076  -0.0148  -0.0023  ** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLos resultados muestran un efecto estadísticamente significativo del año de publicación sobre la prevalencia del evento \\((p = 0,008)\\).\nLos resultados de la metarregresión se visualizan usando bubble plots. Estos gráficos representan en el eje \\(X\\) el valor del moderador continuo y en el eje \\(Y\\) el efecto estimado para cada estudio. Cada burbuja representa un estudio individual, y su tamaño es proporcional al peso del estudio (generalmente inversamente proporcional a la varianza). Además, el gráfico incluye una línea de regresión con su intervalo de confianza del 95%, permitiendo visualizar la tendencia general.\nLa función bubble() genera este gráfico de forma rápida:\n\nbubble(\n  mod_year,\n  cex = \"common\",        # Escala de los símbolos\n  col.line = pal[2]      # Color de la línea de regresión\n)\n\n\n\n\n\n\n\n\nPara ajustar el tamaño de las burbujas según el peso de cada estudio, se puede modificar el argumento cex utilizando la función rescale() del paquete plotrix:\n\nbubble(\n  mod_year,\n  cex = plotrix::rescale(\n    x = 1 / mod_year$vi,\n    newrange = c(0.5, 3)),  # Ajusta la escala de los símbolos\n    col.line = pal[2]\n)\n\n\n\n\n\n\n\n\n\n\nVisualización avanzada con paquete orchaRd\nEste contenido es opcional y está dirigido a quienes tengan conocimientos más avanzados de R y deseen generar visualizaciones más atractivas y listas para la presentación en informes o artículos científicos.\nEl paquete orchaRd (Nakagawa et al. 2023) permite crear visualizaciones con mayores opciones de personalización y compatibles con ggplot2. Sin embargo, dado que orchaRd no permite utilizar modelos ajustados por el paquete meta, es necesario reajustar el modelo utilizando metafor.\nCargamos los paquetes requeridos:\n\n# Carga paquetes\nlibrary(orchaRd)\nlibrary(metafor)\n\nCalculamos los estimadores de efecto individuales utilizando la función escalc():\n\ndat_forest &lt;- escalc(\n  measure = \"PLO\",     # estimador de efecto\n  xi = cases,          # número de casos\n  ni = total,          # tamaño muestral\n  data = datos_prev    # conjunto de datos\n  )  \n\nAjustamos el modelo de meta-análisis con moderadores utilizando rma.mv():\n\nmod_forest &lt;- rma.mv(\n  yi = yi,                             # estimador de efecto\n  V = vi,                              # error estándar\n  mods = ~ factor(pais_cat) + pubyear, # moderadores\n  random = ~ 1|study,                  # modelo de efectos aleatorios\n  data = dat_forest                    # conjunto de datos\n)\n\nGeneramos el forest plot con la función orchard_plot(), definiendo los siguientes argumentos:\n\ngroup: identificador de estudio.\nmod: nombre del moderador (1 para estimador de efecto global).\nxlab: nombre del estimador de efecto.\ntransf: mostrar los datos en escala original (opcional).\n\n\n# Forest plot por subgrupos\norchard_plot(\n  mod_forest, \n  group = \"study\",\n  mod = \"pais_cat\",\n  xlab = \"prevalencia\",\n  transfm = \"invlogit\"\n  ) +\n  # Paleta personalizada\n  scale_color_manual(values = pal) +  # color de borde\n  scale_fill_manual(values = pal)     # color de relleno\n\n\n\n\n\n\n\n\nCon el argumento scale_fill_scico_d() podemos usar cualquiera de las paletas colorblind-friendly incluidas en scico:\n\n# Forest plot por subgrupos\norchard_plot(\n  mod_forest, \n  group = \"study\",\n  mod = \"pais_cat\",\n  xlab = \"prevalencia\",\n  transfm = \"invlogit\"\n  ) +\n  # Paleta colorblind-friendly\n  scale_color_scico_d(palette = \"hawaii\") + # color de borde\n  scale_fill_scico_d(palette = \"hawaii\")    # color de relleno\n\n\n\n\n\n\n\n\nGeneramos el bubble plot para el moderador continuo (pubyear):\n\nbubble_plot(\n  mod_forest,           # modelo ajustado con metafor\n  transfm = \"invlogit\", # muestra resultados como proporciones\n  mod = \"pubyear\",      # moderador continuo\n  group = \"study\",      # identificador de estudio\n  est.col = pal[2],     # color de la línea de regresión\n  ci.col = pal[1],      # color de las líneas del 95% IC\n  ) \n\n\n\n\n\n\n\n\nSi quisiera generar un bubble plot para cada nivel de pais_cat, puedo usar el argumento by:\n\nbubble_plot(\n  mod_forest,           # modelo ajustado con metafor\n  transfm = \"invlogit\", # muestra resultados como proporciones\n  mod = \"pubyear\",      # moderador continuo\n  group = \"study\",      # identificador de estudio\n  est.col = pal[2],     # color de la línea de regresión\n  ci.col = pal[1],      # color de las líneas del 95% IC\n  by = \"pais_cat\"\n  ) +\n  \n  # Paleta colorblind-friendly\n  scale_color_scico_d(palette = \"glasgow\") + # color de borde\n  scale_fill_scico_d(palette = \"glasgow\")    # color de relleno"
  },
  {
    "objectID": "mod_bias.html#sesgo-de-publicación-y-análisis-de-sensibilidad",
    "href": "mod_bias.html#sesgo-de-publicación-y-análisis-de-sensibilidad",
    "title": "Exploración de la heterogeneidad",
    "section": "Sesgo de publicación y análisis de sensibilidad",
    "text": "Sesgo de publicación y análisis de sensibilidad\nEl sesgo de publicación (publication bias) se refiere a la tendencia de publicar con mayor frecuencia estudios con resultados positivos, estadísticamente significativos o con grandes tamaños del efecto. Esto puede distorsionar la síntesis de la evidencia en un meta-análisis, produciendo una sobreestimación del efecto global. Por ello, es esencial evaluar y ajustar el sesgo de publicación para garantizar que los resultados sean lo más precisos y representativos posible.\n\nFunnel plot\nEl funnel plot o gráfico de embudo representa en el eje \\(Y\\) el tamaño muestral o la precisión de los estudios, y en el eje \\(X\\) el estimador de efecto para cada estudio. En ausencia de sesgo de publicación, se espera una distribución simétrica que forme un patrón similar a un embudo; de lo contrario, se observa asimetría.\nPara generar un funnel plot, se utiliza la función funnel():\n\nfunnel(mod_prev)\n\n\n\n\n\n\n\n\n\n\nTest de Egger\nEl test de Egger evalúa la simetría del funnel plot mediante una regresión lineal entre el estimador de efecto y su error estándar. Un p-valor menor a 0,05 sugiere la presencia de sesgo de publicación.\nEste test se implementa con la función metabias():\n\nmetabias(\n  mod_prev,               # modelo de meta-análisis\n  method.bias = \"Egger\"   # aplica test de Egger (opción por defecto)\n  )\n\nLinear regression test of funnel plot asymmetry\n\nTest result: t = -1.45, df = 24, p-value = 0.1603\nBias estimate: -0.6649 (SE = 0.4589)\n\nDetails:\n- multiplicative residual heterogeneity variance (tau^2 = 1.4335)\n- predictor: standard error\n- weight:    inverse variance\n- reference: Egger et al. (1997), BMJ\n\n\n\n\nTest de Begg\nEl test de Begg utiliza la correlación de rangos para evaluar la relación entre el tamaño del efecto y el error estándar. Un p-valor menor que 0,05 indica la presencia de sesgo de publicación.\nSe implementa añadiendo el argumento method.bias = \"Begg\" a la función metabias():\n\nmetabias(\n  mod_prev,             # modelo de meta-análisis\n  method.bias = \"Begg\"  # aplica test de Begg\n  )\n\nRank correlation test of funnel plot asymmetry\n\nTest result: z = -0.90, p-value = 0.3662\nBias estimate: -41.0000 (SE = 45.3689)\n\nReference: Begg & Mazumdar (1993), Biometrics\n\n\n\n\nTrim-and-fill\nEl método trim-and-fill estima el número de estudios faltantes debido al sesgo de publicación y ajusta la media global en consecuencia, incorporando estudios hipotéticos. Esto ayuda a corregir la estimación del efecto global.\nSe utiliza la función trimfill():\n\ntrimfill(mod_prev)\n\nNumber of studies: k = 32 (with 6 added studies)\n\n                      events             95%-CI\nRandom effects model 23.4496 [21.7062; 25.3329]\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.0181; tau = 0.1344; I^2 = 42.4% [12.2%; 62.2%]; H = 1.32 [1.07; 1.63]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 53.82   31  0.0067\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Maximum-likelihood estimator for tau^2\n- Calculation of I^2 based on Q\n- Trim-and-fill method to adjust for funnel plot asymmetry (L-estimator)\n- Logit transformation\n- Events per 100000 observations\n\n\nGeneralmente, se recomienda utilizar dos o más métodos para evaluar el sesgo de publicación y obtener una visión más completa de su impacto en el meta-análisis. En el ejemplo anterior, no hubiera sido necesario realizar el trim-and-fill ya que el funnel plot tiene una forma bastante simétrica y el p-valor de los test de Egger y Begg fue menor a 0,05.\n\n\nAnálisis de sensibilidad\nEl análisis de sensibilidad se utiliza para evaluar la robustez de los resultados del meta-análisis. Una estrategia común es reajustar el modelo excluyendo estudios con menor tamaño muestral o de menor calidad, y comparar el estimador de efecto y su 95% IC con el modelo original. Otra opción es realizar un análisis leave-one-out, en el que se remueve un estudio a la vez y se observa la variación del efecto global.\nLa función metainf() permite realizar el análisis de influencia (leave-one-out):\n\nmetainf(\n  mod_prev,          # modelo de meta-análisis\n  pooled = \"random\"  # modelo de efectos aleatorios\n  )\n\nEste análisis ayuda a determinar si algún estudio en particular influye de forma excesiva en los resultados del meta-análisis. Al igual que en los modelos anteriores, los resultados pueden visualizarse usando forest plots.\n\n\n\n\n\n\nHasta aquí hemos cubierto los aspectos básicos para evaluar fuentes de heterogeneidad en modelos de meta-análisis. Quienes tengan interés en profundizar en las medidas de heterogeneidad y su aplicación, recomendamos consultar el artículo de Bown y Sutton (2010) y los capítulos 7, 8 y 9 de Harrer et al. (2021)."
  },
  {
    "objectID": "intro_inferencia.html",
    "href": "intro_inferencia.html",
    "title": "Introducción a la inferencia estadística",
    "section": "",
    "text": "Este material es parte del curso interno de Introducción a la Revisión Sistemática con Meta-análisis  © 2025 Instituto Nacional de Epidemiología “Dr. Juan H. Jara” (ANLIS) -  CC BY-NC 4.0",
    "crumbs": [
      "Material suplementario",
      "Introducción a la inferencia estadística"
    ]
  },
  {
    "objectID": "intro_inferencia.html#fundamentos",
    "href": "intro_inferencia.html#fundamentos",
    "title": "Introducción a la inferencia estadística",
    "section": "Fundamentos",
    "text": "Fundamentos\nLa estadística inferencial es la rama de la estadística que permite extraer conclusiones sobre una población a partir de una muestra de datos. Este proceso se sustenta en dos procedimientos principales: la estimación y la prueba de hipótesis.\nLa población se define como el conjunto completo de individuos u observaciones de interés, mientras que la muestra es el subconjunto representativo de esa población, diseñado para reflejar sus características fundamentales. Para describir la población se utilizan parámetros, valores numéricos como la media poblacional \\((\\mu)\\), mientras que los datos muestrales se resumen con estadísticos, por ejemplo, la media muestral \\((\\bar{x})\\).",
    "crumbs": [
      "Material suplementario",
      "Introducción a la inferencia estadística"
    ]
  },
  {
    "objectID": "intro_inferencia.html#estimación-de-parámetros",
    "href": "intro_inferencia.html#estimación-de-parámetros",
    "title": "Introducción a la inferencia estadística",
    "section": "Estimación de parámetros",
    "text": "Estimación de parámetros\nLa estimación consiste en utilizar información muestral para inferir el valor de un parámetro poblacional. Se distingue entre la estimación puntual, que proporciona un único valor estimado (por ejemplo, \\(\\bar{x}\\) como estimador de \\(\\mu\\)), y la estimación por intervalos de confianza, que define un rango de valores en el cual se espera, con un nivel de confianza dado (habitualmente 95%), que se encuentre el parámetro. La fórmula típica del intervalo de confianza para la media es:\n\\[\n95\\% IC = (\\bar{x}  -1,96~SE, \\bar{x} + 1,96~SE)\n\\]\ndonde \\(SE\\) es el error estándar de \\(\\bar{x}\\) y 1.96 es el valor crítico derivado de la distribución normal (o t de Student en muestras pequeñas o cuando se desconoce la desviación estándar poblacional). Si bien existen ICs del 99%, un mayor nivel de confianza requiere una muestra más grande para mantener un intervalo estrecho y, de esta forma, lograr una mayor precisión en la estimación. El 95% de confianza representa un equilibrio entre el ancho del intervalo (precisión) y el tamaño muestral.",
    "crumbs": [
      "Material suplementario",
      "Introducción a la inferencia estadística"
    ]
  },
  {
    "objectID": "intro_inferencia.html#pruebas-de-hipótesis",
    "href": "intro_inferencia.html#pruebas-de-hipótesis",
    "title": "Introducción a la inferencia estadística",
    "section": "Pruebas de Hipótesis",
    "text": "Pruebas de Hipótesis\nLas pruebas de hipótesis (también conocidas como test o contrastes de hipótesis) permiten evaluar afirmaciones sobre parámetros poblacionales a partir de datos muestrales.\nCuando hablamos de hipótesis, es útil distinguir entre dos tipos principales:\n\nHipótesis de investigación: representan la pregunta o problema que motiva el estudio.\nHipótesis estadística: es la formulación cuantificable que se evalúa mediante métodos de inferencia estadística.\n\nEl contraste de hipótesis se basa en la comparación de dos hipótesis estadísticas:\n\nHipótesis nula \\((H_0)\\), que postula que no existen diferencias entre los grupos comparados (por ejemplo, \\(\\mu = \\mu_0\\)​).\nHipótesis alternativa \\((H_1)\\), que plantea que existen diferencias entre grupos (por ejemplo, \\(\\mu \\neq \\mu_0,~ \\mu &gt; \\mu_0~ ó~ \\mu &lt; \\mu_0\\)). Es equivalente a nuestra hipótesis de investigación y no puede aceptarse ni rechazarse de forma directa.\n\nLa hipótesis que se pone a prueba es \\(H_0\\). Para decidir si se la acepta o rechaza, se utilizan estadísticos de prueba que se comparan con un valor crítico determinado por el nivel de significancia \\(\\alpha\\) (típicamente 0.05).\n\nEstadístico de prueba\nEs el valor calculado a partir de los datos muestrales que permite tomar una decisión sobre \\(H_0\\). Su elección depende del tipo de variable y del problema en estudio. Por ejemplo:\n\nPara variables categóricas se utiliza el estadístico chi-cuadrado \\((\\chi^2)\\).\nPara variables numéricas, se emplean las distribuciones normal \\((Z)\\) o t de Student \\((t)\\).\n\n\n\nValor crítico\nSe denomina región crítica o región de rechazo al conjunto de valores del estadístico de prueba que llevan al rechazo de \\(H_0\\).\nEl nivel de significancia (\\(\\alpha\\)) representa la probabilidad de que el estadístico caiga en la región crítica cuando \\(H_0\\) es verdadera, lo cual constituye un error de tipo I.\nEl valor crítico es el umbral que separa los valores que llevan al rechazo de \\(H_0\\) de aquellos que no. Representa la probabilidad de obtener un resultado igual o más extremo que el observado, bajo la suposición de que \\(H_0\\) es verdadera. En una prueba de hipótesis, es el menor valor de \\(\\alpha\\) para el cual \\(H_0\\) puede rechazarse.\nPara ilustrar la región crítica, se suele usar la gráfica de la distribución normal, donde el p-valor corresponde al área bajo la curva en la región de interés.\n\nEn pruebas unilaterales, se considera el área bajo una sola cola de la distribución. El valor crítico es 1,645 ó -1,645, lo que implica que hay un 5% de probabilidad de observar un valor mayor (o menor) que ese umbral si \\(H_0\\) es verdadera.\n\nTest de cola izquierda \\((H_1: \\mu &lt; \\kappa)\\)\n\n\n\n\n\n\n\n\n\nTest de cola derecha \\((H_1: \\mu &gt; \\kappa)\\)\n\n\n\n\n\n\n\n\n\n\nEn pruebas bilaterales \\((H_1: \\mu \\neq \\kappa)\\), se reparte el 5% en ambas colas de la distribución, y el valor crítico es 1,96. Esto indica que hay un 2,5% de probabilidad en cada cola de obtener un valor más extremo si \\(H_0\\) es verdadera.\n\n\n\n\n\n\n\n\n\n\n\n\np-valor\nEl p-valor representa la probabilidad de obtener un resultado igual o más extremo que el observado, bajo la suposición de que \\(H_0\\) es verdadera. Es decir, el p-valor indica cuán compatibles son los datos con la hipótesis nula. En un test de hipótesis, se rechaza \\(H_0\\) cuando el p-valor es menor que el nivel de significancia \\(\\alpha\\), lo que apoya la hipótesis alternativa (\\(H_1\\)).\n\n\nPotencia estadística\nLa potencia estadística es la probabilidad de rechazar \\(H_0\\)​ cuando esta es falsa, es decir, de detectar un efecto real. Se calcula como 1 menos la probabilidad de cometer un error de Tipo II o \\(\\beta\\) y es directamente proporcional al tamaño muestral y inversamente proporcional a la varianza de las observaciones.",
    "crumbs": [
      "Material suplementario",
      "Introducción a la inferencia estadística"
    ]
  },
  {
    "objectID": "intro_inferencia.html#aplicaciones-e-interpretación",
    "href": "intro_inferencia.html#aplicaciones-e-interpretación",
    "title": "Introducción a la inferencia estadística",
    "section": "Aplicaciones e Interpretación",
    "text": "Aplicaciones e Interpretación\nLa inferencia estadística permite responder preguntas de investigación tales como:\n\n¿Es significativa la diferencia entre dos medias?\n¿Existe una relación entre dos variables?\n¿Cómo se distribuyen los datos respecto a un parámetro de interés?\n\nAl aplicar estos métodos, es crucial tener en cuenta la calidad y representatividad de la muestra, así como la validez de las asunciones subyacentes (normalidad, homogeneidad de varianzas, etc.).\n\n\n\n\n\n\nEste apunte sintetiza los conceptos esenciales y las herramientas básicas para llevar a cabo un análisis inferencial, que sirve de base para la interpretación de modelos y resultados en análisis cuantitativos. Quienes necesiten profundizar más en los temas, les recomendamos consultar las siguientes fuentes:\n\nManual de Epidemiología: Fundamentos, Métodos y Aplicaciones (Epidemiología 2015): Capítulo 3.\nEstadística 12A Edición (Triola 2018): Capítulos 8 y 9.",
    "crumbs": [
      "Material suplementario",
      "Introducción a la inferencia estadística"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portada",
    "section": "",
    "text": "Este material es parte del curso interno de Introducción a la Revisión Sistemática con Meta-análisis  © 2025 Instituto Nacional de Epidemiología “Dr. Juan H. Jara” (ANLIS) -  CC BY-NC 4.0      \n\n\n\n\n\nLes damos la bienvenida al curso de “Introducción a la Revisión Sistemática con Meta-análisis”. Antes de profundizar en los temas específicos, recomendamos a quienes lo requieran repasar el material introductorio sobre R y tidyverse, ya que les proporcionará una base sólida para aprovechar al máximo el curso. ¡Esperamos que disfruten y enriquezcan sus conocimientos!\n\n\n\n Volver arriba",
    "crumbs": [
      "Portada"
    ]
  },
  {
    "objectID": "fixed_random.html",
    "href": "fixed_random.html",
    "title": "Modelos de efectos fijos y aleatorios",
    "section": "",
    "text": "Este material es parte del curso interno de Introducción a la Revisión Sistemática con Meta-análisis  © 2025 Instituto Nacional de Epidemiología “Dr. Juan H. Jara” (ANLIS) -  CC BY-NC 4.0"
  },
  {
    "objectID": "fixed_random.html#introducción",
    "href": "fixed_random.html#introducción",
    "title": "Modelos de efectos fijos y aleatorios",
    "section": "Introducción",
    "text": "Introducción\nUno de los objetivos principales del modelado estadístico es representar la realidad de la manera más “sencilla” posible, capturando su estructura esencial y descartando elementos cuya variabilidad podría generar ruido en la interpretación de los fenómenos.\nPara ajustar un modelo estadístico, partimos de los datos disponibles y buscamos construir una representación basada en ellos. En el caso de los modelos de meta-análisis, los datos de interés son los estimadores de efecto obtenidos en cada estudio, y el objetivo principal es analizar la variabilidad entre ellos, la cual puede deberse a diferencias metodológicas, características de las poblaciones estudiadas u otras fuentes.\nExisten dos enfoques principales en meta-análisis: los modelos de efectos fijos y los modelos de efectos aleatorios. Durante este curso, describiremos sus características fundamentales y su implementación en R. Para quienes deseen profundizar en los fundamentos matemáticos de estos modelos, recomendamos consultar el Capítulo 4 de Schwarzer, Carpenter, y Rücker (2015) y el Capítulo 2 de Harrer et al. (2021)."
  },
  {
    "objectID": "fixed_random.html#modelos-de-efectos-fijos",
    "href": "fixed_random.html#modelos-de-efectos-fijos",
    "title": "Modelos de efectos fijos y aleatorios",
    "section": "Modelos de efectos fijos",
    "text": "Modelos de efectos fijos\nEl modelo de efectos fijos parte de la premisa de que todos los estimadores de efecto incluidos en el meta-análisis corresponden a un mismo efecto verdadero común y que las diferencias observadas entre ellos se deben exclusivamente al error muestral. Bajo este enfoque, se asume que existe un único valor subyacente para el efecto de interés, por lo que el objetivo principal es estimarlo a partir de los datos disponibles.\nEl estimador de efecto global, denotado como \\(\\theta\\), se calcula como un promedio ponderado de los estimadores individuales, asignando a cada estudio un peso proporcional a su precisión:\n\\[\n\\theta = \\frac{\\sum{y_i w_i}}{\\sum{w_i}}\n\\]\nDonde:\n\n\\(y_i\\) es el efecto estimado para cada estudio.\n\\(w_i\\) es el peso asignado a cada estudio, calculado como el inverso de la varianza del estimador de efecto: \\(w_i = 1/S^2_{y_i}\\).\n\nDe este modo, los estudios con menor varianza tienen mayor influencia en la estimación final. Esta estrategia de ponderación se conoce como el método de la varianza inversa.\nDado que estos modelos suponen homogeneidad entre los estudios, no contemplan la existencia de fuentes adicionales de variabilidad más allá del error aleatorio. Por esta razón, también se les conoce como modelos de efecto común (common effect model) o modelos de efectos equivalentes (equal effect model).\nSin embargo, en la práctica, es común encontrar heterogeneidad entre los estudios, lo que puede hacer que el modelo de efectos fijos sea inadecuado. Cuando existe variabilidad real entre los estudios, un modelo de efectos aleatorios suele ser una opción más apropiada, ya que permite incorporar esta variabilidad en la estimación del efecto global."
  },
  {
    "objectID": "fixed_random.html#modelos-de-efectos-aleatorios",
    "href": "fixed_random.html#modelos-de-efectos-aleatorios",
    "title": "Modelos de efectos fijos y aleatorios",
    "section": "Modelos de efectos aleatorios",
    "text": "Modelos de efectos aleatorios\nLos modelos de efectos aleatorios suponen que, además del error aleatorio, existen otras fuentes de variabilidad entre los estudios. En este caso, no se asume que todos los estudios comparten un único efecto verdadero común, sino que cada uno estima un efecto específico que varía dentro de una distribución de efectos verdaderos.\nBajo este enfoque, los efectos verdaderos no son idénticos entre estudios, sino que siguen una distribución alrededor de una media global. Esto introduce una componente adicional de variabilidad, conocida como heterogeneidad entre estudios, que refleja diferencias sistemáticas más allá del error muestral.\nEl objetivo de estos modelos es estimar la media de la distribución de efectos verdaderos, considerando tanto la variabilidad dentro de cada estudio como la variabilidad entre estudios. Para ello, el peso asignado a cada estudio en la estimación global se ajusta de la siguiente manera:\n\\[w^*_i = \\frac{1}{S^2_{y_i} + \\tau^2}\\]\ndonde:\n\n\\(S^2_{y_i}\\) es la varianza intraestudio, que indica la precisión del estimador de efecto.\n\\(\\tau^2\\) (tau-cuadrado) es la varianza entre estudios, que mide la heterogeneidad en los efectos estimados.\n\nExisten diversos métodos para estimar \\(\\tau^2\\), siendo los más comunes la máxima verosimilitud restringida (REML) y el método de DerSimonian y Laird. Aunque el desarrollo matemático de estos métodos excede los alcances del curso, su correcta aplicación es fundamental para interpretar adecuadamente los resultados de un meta-análisis con efectos aleatorios."
  },
  {
    "objectID": "fixed_random.html#medidas-de-heterogeneidad",
    "href": "fixed_random.html#medidas-de-heterogeneidad",
    "title": "Modelos de efectos fijos y aleatorios",
    "section": "Medidas de heterogeneidad",
    "text": "Medidas de heterogeneidad\nEn un meta-análisis, la variabilidad en los resultados puede deberse a múltiples fuentes:\n\nVariabilidad intraestudio, que refleja las diferencias entre los participantes dentro de cada estudio.\nHeterogeneidad entre estudios, es decir, la variabilidad en los efectos estimados más allá de lo esperable por azar.\nError de muestreo y otras fuentes de incertidumbre, que pueden influir en las diferencias observadas.\n\nLa heterogeneidad entre estudios es particularmente importante, ya que indica si los efectos varían más de lo que se esperaría solo por error aleatorio. En la siguiente tabla, adaptada de Schwarzer, Carpenter, y Rücker (2015), se resumen los indicadores más utilizados para cuantificar la heterogeneidad:\n\n\n\nMedida\nInterpretación\nEscala\nRango\nNúmero de estudios\nAfectada por precisión\n\n\n\n\nQ de Cochran\nEvalúa si la heterogeneidad observada es estadísticamente significativa.\nAbsoluta\n[0, ∞)\nDependiente\nSí\n\n\nI²\nCuantifica qué proporción de la variabilidad total se debe a heterogeneidad real.\nPorcentaje\n[0, 100]\nIndependiente\nSí\n\n\nτ²\nMide la variabilidad real entre los efectos verdaderos de los estudios.\nVarianza\n[0, ∞)\nIndependiente\nNo\n\n\nH²\nRazón entre la varianza total observada y la varianza esperada bajo el supuesto de homogeneidad.\nAbsoluta\n[1, ∞)\nIndependiente\nSí\n\n\n\nEstos indicadores permiten evaluar si las diferencias observadas entre los estudios justifican el uso de un modelo de efectos aleatorios en lugar de uno de efectos fijos."
  },
  {
    "objectID": "fixed_random.html#implementación-en-r",
    "href": "fixed_random.html#implementación-en-r",
    "title": "Modelos de efectos fijos y aleatorios",
    "section": "Implementación en R",
    "text": "Implementación en R\nPara ajustar modelos de meta-análisis en R, los dos paquetes más utilizados son metafor (Viechtbauer 2010) y meta (Balduzzi, Rücker, y Schwarzer 2019).\n\nmetafor: Es un paquete flexible y potente que permite modelar escenarios complejos con alta precisión. Sin embargo, su uso requiere una curva de aprendizaje más pronunciada y un conocimiento avanzado en modelado estadístico.\nmeta: Es más accesible y fácil de usar, lo que lo convierte en una excelente opción para quienes sólo poseen conocimientos básicos de estadística inferencial y manejo de R. Este paquete es ideal para aplicaciones prácticas y proporciona una interfaz más simple y directa para realizar análisis estándar.\n\nDado que este curso se enfoca en la aplicación práctica del meta-análisis, utilizaremos principalmente el paquete meta. Este paquete ajusta de manera predeterminada tanto modelos de efectos fijos como de efectos aleatorios e incluye distintos estimadores de heterogeneidad estadística.\n\nEstructura básica\nEl paquete meta ofrece una serie de funciones para ajustar modelos de meta-análisis con una estructura uniforme. La función principal sigue el formato metaxxx(), donde xxx indica el estimador de efecto a calcular. Sus argumentos clave incluyen:\n\nmetaxxx(\n  studlab,\n  data,\n  sm,\n  common,\n  random,\n  method.tau,\n  backtransf,\n  subset,        \n  exclude,\n  subgroup,\n  cluster        \n)\n\ndonde:\n\nstudlab: Identificador único de cada estudio en el conjunto de datos.\ndata: Conjunto de datos con los resultados de los estudios incluidos en el meta-análisis.\nsm: Estimador de efecto global a calcular (ej.: “OR” para odds ratio, “RR” para riesgo relativo, “MD” para diferencia de medias, etc.).\ncommon: Indica si se ajusta un modelo de efectos fijos (TRUE, por defecto) o se omite (FALSE).\nrandom: Indica si se ajusta un modelo de efectos aleatorios (TRUE, por defecto) o se omite (FALSE).\nmethod.tau: Método para estimar la varianza entre estudios (REML, por defecto).\nbacktransf: Define si los resultados se muestran en la escala original de los datos (TRUE) o transformada (FALSE, por ejemplo log-OR).\nsubset: Permite seleccionar un subconjunto de estudios para el análisis (opcional).\nexclude: Permite excluir estudios específicos (opcional).\nsubgroup: Permite realizar un análisis por subgrupos (opcional).\ncluster: Permite ajustar modelos multinivel si los datos están agrupados en clústeres (opcional).\n\n\n\nEjemplo práctico\nEn este ejemplo, utilizaremos la función metagen(), diseñada para trabajar con estimadores de efecto previamente calculados. Usaremos el conjunto de datos dat.konstantopoulos2011, que contiene información de un meta-análisis de 56 estudios sobre el impacto de la modificación del calendario escolar en el rendimiento académico. Este conjunto de datos forma parte de la dependencia metadat (Viechtbauer et al. 2025), que se carga automáticamente al activar el paquete meta.\n\n# Cargar el paquete meta\nlibrary(meta)\n\n# Cargar datos\ndatos &lt;- dat.konstantopoulos2011\n\n# Explorar variables disponibles\nnames(datos)\n\n[1] \"district\" \"school\"   \"study\"    \"year\"     \"yi\"       \"vi\"      \n\n\nLas variables de entrada para metagen() serán yi (diferencia de medias estandarizada) y vi (varianza de la estimación). Ajustamos los modelos de efectos fijos y aleatorios:\n\n# Ajustar el modelo de efectos fijos y aleatorios\nmod &lt;- metagen(TE = yi,\n               seTE = vi,\n               studlab = study,\n               common = TRUE,    \n               random = TRUE,    \n               backtransf = TRUE, \n               data = datos)\n\n# Mostrar salida del modelo\nmod\n\nNumber of studies: k = 56\n\n                                         95%-CI      z p-value\nCommon effect model  -0.0133 [-0.0140; -0.0126] -38.82       0\nRandom effects model  0.1219 [ 0.0365;  0.2074]   2.80  0.0052\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.1048 [0.0739; 0.1588]; tau = 0.3238 [0.2719; 0.3985]\n I^2 = 99.9%; H = 41.32\n\nTest of heterogeneity:\n        Q d.f. p-value\n 93892.81   55       0\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n\n\nDado que la heterogeneidad estadística es alta \\((I^2 = 99,9\\%)\\), podemos omitir el modelo de efectos fijos cambiando a FALSE el argumento common:\n\n# Ajustar solo el modelo de efectos aleatorios\nmod &lt;- metagen(TE = yi,\n               seTE = vi,\n               studlab = study,\n               common = FALSE,    \n               random = TRUE,    \n               backtransf = TRUE, \n               data = datos)\n\n# Mostrar salida del modelo\nmod\n\nNumber of studies: k = 56\n\n                                      95%-CI    z p-value\nRandom effects model 0.1219 [0.0365; 0.2074] 2.80  0.0052\n\nQuantifying heterogeneity (with 95%-CIs):\n tau^2 = 0.1048 [0.0739; 0.1588]; tau = 0.3238 [0.2719; 0.3985]\n I^2 = 99.9%; H = 41.32\n\nTest of heterogeneity:\n        Q d.f. p-value\n 93892.81   55       0\n\nDetails of meta-analysis methods:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Calculation of I^2 based on Q\n\n\nPara explorar los resultados con más detalle, podemos utilizar summary(mod) que incluye las métricas para cada estudio individual:\n\nEstimador de efecto: Diferencia de medias estandarizada por estudio.\n95%-CI: Intervalo de confianza al 95% de la diferencia de medias estandarizada.\n%W (random): Peso de cada estudio en el modelo de efectos aleatorios, determinado por el tamaño muestral y la varianza.\n\nA continuación se muestran los resultados generales del meta-análisis:\n\nk: número total de estudios incluidos en el análisis.\nCommon effects model: Diferencia de medias observada para cada estudio en el modelo de efectos fijos.\nRandom effects model: Diferencia de medias observada para cada estudio en el modelo de efectos aleatorios.\nEl estadístico z y su p-valor (p-value) para evaluar la significancia del efecto global.\n\nLuego se presentan las métricas de heterogeneidad:\n\ntau^2 y tau: cuantifican la variabilidad entre estudios más allá del error muestral.\nI^2: porcentaje de variabilidad atribuida a diferencias reales entre estudios.\nH y Q: indicadores de heterogeneidad en el conjunto de estudios.\n\nLa salida del modelo también detalla los métodos estadísticos utilizados, incluyendo:\n\nMétodo de varianza inversa para ponderar los estudios.\nEstimador de máxima verosimilitud restringida para tau^2.\nMetodología aplicada para calcular I^2.\n\nEn base a la salida anterior, podemos concluir que el meta-análisis realizado sobre 56 estudios individuales muestra que el rendimiento académico aumenta significativamente con la modificación del calendario escolar \\((p &lt; 0.005)\\). La alta heterogeneidad estadística \\((I^2 = 99,9\\%)\\) sugiere que la variabilidad observada se debe a diferencias reales entre estudios.\n\n\nForest plots\nLos resultados del meta-análisis pueden visualizarse mediante forest plots, gráficos que representan la distribución de los estimadores de efecto de los estudios individuales y sus intervalos de confianza en relación con el estimador global. Además, proporcionan información sobre la heterogeneidad entre estudios, facilitando la interpretación de los resultados.\nEl paquete meta incluye la función forest(), que permite generar forest plots de forma rápida y con múltiples opciones de personalización. Para conocer todos los argumentos disponibles, se puede ejecutar ?forest en la consola de R.\nAlgunos de los principales argumentos de forest() incluyen:\n\nforest(\n  mod, # Nombre del modelo de meta-análisis\n  sortvar, # Ordena los estudios según una variable numérica\n  smlab, # Etiqueta del estimador de efecto\n  col.diamond, # Color del estimador global\n  col.square, # Color de los estimadores individuales\n  print.tau2 = TRUE, # Tau-cuadrado (TRUE por defecto)\n  print.I2 = TRUE, # I-cuadrado (TRUE por defecto)\n  print.Q = TRUE, # Estadístico Q de Cochran (TRUE por defecto)\n  print.pval.Q = TRUE, # p-valor del estadístico Q (TRUE por defecto)\n  digits = 2, # Número de decimales a mostrar\n  ...)\n\nA continuación, generamos un forest plot básico a partir del modelo de efectos aleatorios. Para una mejor visualización, vamos a personalizar los colores del gráfico con los argumentos col.diamond y col.square:\n\nforest(mod,\n       smlab = \"Diferencia de medias estandarizada\",\n       col.diamond = \"orange\",\n       col.square = \"turquoise\")\n\n\n\n\n\n\n\n\nEl gráfico generado se organiza en tres paneles principales:\n\nPanel izquierdo:\n\nIdentificador de estudio (\"studlab\")\nColumnas adicionales dependientes del estimador de efecto utilizado.\n\nPanel central:\n\nLínea vertical de referencia que indica el valor de no efecto (0 en datos continuos, 1 en escalas logarítmicas).\nLínea punteada que representa el estimador global del meta-análisis.\nRombo (🔶): Representa el estimador global, cuyo ancho indica el intervalo de confianza al 95%.\nCuadrados (🟦): Representan los estimadores de los estudios individuales, con un tamaño proporcional al peso del estudio en el análisis.\nBigotes horizontales: Indican los intervalos de confianza al 95% de cada estudio.\n\nPanel derecho:\n\nEstimador de efecto e intervalo de confianza al 95% de cada estudio.\nPeso estadístico asignado a cada estudio en el modelo de efectos aleatorios.\n\n\nSe puede controlar la información que aparece en los lados del forest plot mediante los argumentos leftcols, rightcols, leftlabs y rightlabs. También es posible aplicar formatos predefinidos con layout = \"RevMan5\" o layout = \"JAMA\", que ajustan el diseño según estilos ampliamente utilizados en la literatura científica.\nLos gráficos generados con forest() no son compatibles conggplot2 ni se autoescalan, lo que puede ser problemático si el número de estudios es grande, ya que el gráfico podría quedar ilegible en la vista predeterminada.\nPara evitar este problema, se recomienda exportar el gráfico a un archivo de imagen (por ejemplo, PDF o PNG) usando las funciones pdf() o png(), especificando un tamaño adecuado antes de ejecutarlo con forest().\n\n# Defino parámetros para guardar la imagen\npng(\n  filename = \"forest.png\", # Nombre de archivo para guardar el gráfico\n  width = 8, # Ancho del gráfico (ajustar según sea necesario)\n  height = 13, # Alto del gráfico (ajustar según sea necesario)\n  units = \"in\", # Unidad para definir el tamaño\n  res = 300, # Resolución de imagen\n  )\n\n# Genero el gráfico (no se visualiza en el panel de Plots)\nforest(\n  mod,\n  smlab = \"Diferencia de medias estandarizada\",\n  col.diamond = \"orange\",\n  col.square = \"turquoise\",\n  leftcols = \"studlab\"\n)\n\n# Guardo el gráfico\ndev.off()\n\n\n\n\n\n\n\nEn la siguiente parte de esta unidad, exploraremos las funciones de meta que permiten ajustar modelos de meta-análisis para distintos estimadores de efecto en epidemiología. Además, abordaremos métodos para controlar la heterogeneidad, tales como el análisis de moderadores y la meta-regresión y aprenderemos qué es y como se mide el sesgo de publicación."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introducción",
    "section": "",
    "text": "Este material es parte del curso interno de Introducción a la Revisión Sistemática con Meta-análisis  © 2025 Instituto Nacional de Epidemiología “Dr. Juan H. Jara” (ANLIS) -  CC BY-NC 4.0"
  },
  {
    "objectID": "intro.html#qué-es-un-meta-análisis",
    "href": "intro.html#qué-es-un-meta-análisis",
    "title": "Introducción",
    "section": "¿Qué es un meta-análisis?",
    "text": "¿Qué es un meta-análisis?\nUn meta-análisis es una herramienta estadística que permite sintetizar cuantitativamente la evidencia de investigaciones independientes sobre un mismo problema de investigación. Se lo ha definido como un “análisis de análisis” (Glass 1976), ya que su unidad de análisis no son individuos o poblaciones, sino estudios científicos.\nEl objetivo principal de un meta-análisis es proporcionar un estimador numérico que resuma los resultados de los estudios incluidos, permitiendo evaluar la magnitud del efecto de una intervención o la relación entre variables en diferentes contextos (Harrer et al. 2021). Es importante destacar que los meta-análisis son adecuados únicamente para investigaciones cuantitativas y, en general, requieren que los estudios analizados compartan el mismo diseño y estimen medidas de asociación similares.\nA continuación, se presentan algunas de las principales ventajas y limitaciones del meta-análisis:\n\n\n\n\n\nVentajas\nDesventajas\n\n\n\n\nPermite una síntesis cuantitativa de la evidencia disponible.\nLa validez de los resultados depende de la calidad metodológica de los estudios incluidos.\n\n\nAumenta la potencia estadística al combinar los datos de múltiples estudios.\nPuede estar afectado por sesgos de publicación.\n\n\nMejora la precisión de los estimadores al reducir la variabilidad aleatoria.\nLa heterogeneidad entre estudios puede dificultar la interpretación de los resultados.\n\n\nPermite identificar patrones no evidentes en estudios individuales.\nRequiere una metodología rigurosa y criterios estrictos de selección de estudios.\n\n\nEvalúa la consistencia de los resultados en diferentes poblaciones y contextos.\nNo corrige errores metodológicos de los estudios primarios."
  },
  {
    "objectID": "intro.html#estimadores-de-efecto",
    "href": "intro.html#estimadores-de-efecto",
    "title": "Introducción",
    "section": "Estimadores de efecto",
    "text": "Estimadores de efecto\nEn estudios individuales, se asume que las variables de interés fueron medidas de manera uniforme en todos los participantes. Esto permite aplicar técnicas de estadística descriptiva, como el análisis exploratorio de datos (EDA), para caracterizar la muestra y evaluar relaciones entre variables.\nSin embargo, en los meta-análisis esta suposición no siempre se cumple. Aunque los criterios de inclusión sean estrictos, los estudios pueden diferir en diseño, población, medición de variables o definición de resultados. Por ello, no es posible sintetizar la evidencia utilizando exclusivamente herramientas de la estadística tradicional.\nPara integrar los resultados de diferentes estudios, los meta-análisis utilizan estimadores de efecto, también llamados tamaño de efecto o effect size. En algunos casos, estos valores se extraen directamente de los artículos, pero a menudo deben calcularse a partir de los datos reportados. Un buen estimador de efecto debe cumplir con las siguientes condiciones:\n\nComparable: Debe ser consistente entre los estudios incluidos.\nComputable: Debe poder calcularse a partir de la información disponible.\nConfiable: Debe permitir estimar su error estándar.\nInterpretable: Debe responder adecuadamente a la pregunta de investigación.\n\nDesde una perspectiva estadística, los estimadores de efecto son equivalentes a los coeficientes en modelos de regresión o a las medidas de asociación en estudios epidemiológicos, ya que cuantifican la fuerza y dirección de la relación entre dos variables. Algunos estimadores de efecto comúnmente utilizados en investigación epidemiológica y aplicables a modelos de meta-análisis incluyen: proporción, incidencia, correlación, diferencia de medias, odds-ratio (OR), riesgo relativo (RR) y hazard-ratio.\nEn las próximas secciones, exploraremos los modelos de meta-análisis más adecuados para cada estimador de efecto y su implementación en software R."
  },
  {
    "objectID": "intro_R.html",
    "href": "intro_R.html",
    "title": "Introducción a R y RStudio",
    "section": "",
    "text": "Este material es parte del curso interno de Introducción a la Revisión Sistemática con Meta-análisis  © 2025 Instituto Nacional de Epidemiología “Dr. Juan H. Jara” (ANLIS) -  CC BY-NC 4.0",
    "crumbs": [
      "Material suplementario",
      "Introducción a R y RStudio"
    ]
  },
  {
    "objectID": "intro_R.html#qué-es-r",
    "href": "intro_R.html#qué-es-r",
    "title": "Introducción a R y RStudio",
    "section": "¿Qué es R?",
    "text": "¿Qué es R?\nR (2025) es un lenguaje de programación interpretado, orientado a objetos, multiplataforma y de código abierto, diseñado específicamente para el análisis estadístico de datos. Cuenta con estructuras y sintaxis propias, y una extensa colección de funciones desarrolladas para aplicaciones estadísticas.\n\nComo lenguaje orientado a objetos, todo lo que manipulamos —variables, funciones, conjuntos de datos, resultados— se considera un objeto, lo que aporta flexibilidad y simplicidad al trabajo con información.\nAl ser un lenguaje interpretado, los scripts se ejecutan directamente sin necesidad de compilación, lo que favorece la exploración interactiva.\nR es multiplataforma: se puede instalar y ejecutar en Linux, Windows y macOS con un comportamiento consistente.\nAdemás, es software libre distribuido bajo licencia GNU-GPL, lo que permite su uso, modificación y redistribución sin restricciones.\n\nPara instalarlo en Windows, se debe descargar el instalador desde el sitio oficial del proyecto R (CRAN) y seguir los pasos guiados. Una vez finalizada la instalación, R estará listo para usarse desde cualquier entorno compatible. Sin embargo, si no se cuenta con experiencia previa en programación, no se recomienda utilizar R directamente desde su consola nativa.",
    "crumbs": [
      "Material suplementario",
      "Introducción a R y RStudio"
    ]
  },
  {
    "objectID": "intro_R.html#qué-es-rstudio",
    "href": "intro_R.html#qué-es-rstudio",
    "title": "Introducción a R y RStudio",
    "section": "¿Qué es RStudio?",
    "text": "¿Qué es RStudio?\nRStudio Desktop (2025, Posit Software) es un entorno de desarrollo integrado (IDE) diseñado específicamente para facilitar el trabajo con R. Proporciona una interfaz unificada que incluye editor de scripts, consola de R, entorno, explorador de archivos, panel de gráficos y ayuda, entre otros, optimizando el flujo de trabajo.\n\n\n\n\n\nEntre sus principales ventajas se encuentran:\n\nAsistente de código: al escribir en el editor o la consola, la tecla Tab activa el autocompletado de funciones, nombres de objetos y argumentos, agilizando la escritura y reduciendo errores de sintaxis. En versiones recientes, el asistente también permite la previsualización de colores en los gráficos, resaltar los paréntesis de cierre en funciones anidadas con distintos colores y gestionar automáticamente la indentación del código.\n\n\n\n\nAyuda en línea: al posicionar el cursor sobre el nombre de una función en el editor y presionar F1, se accede directamente a la documentación correspondiente en el panel Help (habitualmente ubicado en la esquina inferior derecha).\n\nHistorial de comandos: en la consola, al usar las teclas de flecha arriba/abajo, se puede navegar por los comandos ejecutados durante la sesión actual. Además, el panel History (parte superior derecha) almacena los comandos de todas las sesiones previas, permitiendo reutilizarlos con un clic en To Console (Enter) o To Source (Shift + Enter), según se desee insertarlos en la consola o en el script activo.\n\n\nRStudio es multiplataforma, de código abierto, y permite una integración fluida con herramientas del ecosistema R, como R Markdown, Quarto, control de versiones y manejo de proyectos. Para instalarlo, se puede acceder al siguiente enlace: https://posit.co/download/rstudio-desktop/.\n\n\n\n\n\n\nUna vez instalados R y RStudio, ya contamos con todo lo necesario para comenzar a trabajar. Aunque instalamos ambos programas, en la práctica sólo necesitamos abrir RStudio, que utiliza a R como motor de ejecución.\n\n\n\n\nProyectos en RStudio\nLos proyectos de RStudio permiten organizar de forma estructurada todo el material asociado a un análisis: scripts, informes, bases de datos, imágenes, etc. Cada proyecto se vincula a una carpeta específica del sistema de archivos, y RStudio la utiliza como directorio de trabajo por defecto. Esto facilita la importación de datos y evita errores relacionados con rutas relativas o absolutas.\nPara crear un nuevo proyecto, se puede utilizar el menú File &gt; New Project… o el acceso directo New Project… ubicado en la esquina superior derecha de la interfaz. En ambos casos, se abre un asistente con tres opciones:\n\n\n\n\n\n\nNew Directory: crea una nueva carpeta para el proyecto. Es la opción más habitual.\nExisting Directory: vincula el proyecto a una carpeta ya existente que contenga archivos previos.\nVersion Control: permite clonar un repositorio (Git o SVN). Esta opción no se utilizará en este curso.\n\nTrabajar con proyectos garantiza que, al importar archivos, RStudio los busque automáticamente dentro de la carpeta correspondiente. Además, cada proyecto mantiene su propio entorno de trabajo, lo que significa que al cerrar o cambiar de proyecto, se conserva la configuración previa sin interferencias.\nCuando un proyecto ya existe, dentro de la carpeta encontraremos un archivo con extensión .Rproj que al ejecutarlo abre una nueva sesión de RStudio con el proyecto activo. Otras opciones son abrir desde File &gt; Open Project… o desde el ícono  en la esquina superior derecha de RStudio. Esta última opción también mantiene un historial de los proyectos abiertos recientemente, lo que permite acceder rápidamente a ellos mediante accesos directos.\n\n\nScripts en RStudio\nUn script es un archivo de texto plano que contiene instrucciones escritas en R. Permite guardar, reutilizar y compartir el código, favoreciendo la reproducibilidad del análisis.\n\nCrear un nuevo script: podemos crear un script desde el menú File &gt; New File &gt; R Script (acceso rápido: Ctrl + Shift + N) o haciendo clic en el ícono de la hoja (📄) con símbolo “+” en la barra de herramientas.\nEjecutar código: la forma habitual de ejecutar un script es línea por línea, con Ctrl + Enter o el botón Run (). El cursor debe estar en cualquier punto de la línea a ejecutar. Tras la ejecución, el cursor avanza automáticamente a la siguiente línea de código.\nEditar un script: las líneas del script pueden editarse directamente. Cada vez que se realiza una modificación, es necesario volver a ejecutar esas líneas para actualizar los resultados.\nGuardar un script: Para guardar los cambios, se puede usar el ícono del diskette (💾), el menú File &gt; Save, o el atajo Ctrl + S. Para guardar con otro nombre o ubicación, utilizar File &gt; Save As…\nAbrir un script existente: Los archivos de script tienen extensión .R. Pueden abrirse desde el panel File &gt; Open File…, el panel Files o usando el atajo de teclado Ctrl + O. Al abrirse, se muestran en una nueva pestaña del editor.",
    "crumbs": [
      "Material suplementario",
      "Introducción a R y RStudio"
    ]
  },
  {
    "objectID": "intro_R.html#funciones",
    "href": "intro_R.html#funciones",
    "title": "Introducción a R y RStudio",
    "section": "Funciones",
    "text": "Funciones\nEn R, los comandos básicos se denominan funciones. Muchas de ellas están incluidas en el núcleo del lenguaje (conocido como R base) y se denominan integradas, mientras que otras forman parte de paquetes adicionales.\nCada función tiene un nombre y suele requerir uno o más argumentos (también llamados parámetros), que se escriben entre paréntesis y separados por comas. Incluso las funciones que no requieren argumentos deben escribirse con paréntesis vacíos.\n\n# Sintaxis general\nnombre_de_la_función(arg1, arg2, ...)\n\nLas funciones siempre ejecutan una acción o devuelven un valor, que puede ser visualizado, almacenado o utilizado en otras operaciones.\n\nReglas de sintaxis\nDado que R es un lenguaje interpretado, la sintaxis debe ser estrictamente correcta. Algunos puntos clave:\n\nLos argumentos pueden escribirse con el nombre del parámetro seguido de un signo igual:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuncion(arg1 = 32, arg2 = 5, arg3 = 65)\n```\n:::\n\nTambién se pueden omitir los nombres y escribir directamente los valores. En ese caso, el orden importa y debe coincidir con el definido en la documentación de la función:\n\nfuncion(32, 5, 65)\n\n\n\n\nTipos de argumentos\nLos argumentos pueden ser:\n\nValores numéricos: 3, 10.5\nLógicos: TRUE, FALSE\nEspeciales: NA (faltante), NULL, Inf\nTexto: debe escribirse entre comillas, por ejemplo \"menos\"\nObjetos: como variables previamente creadas (x, datos, etc.)\n\nfuncion(arg1 = 3, arg2 = NA, arg3 = TRUE, arg4 = \"menos\", arg5 = x)",
    "crumbs": [
      "Material suplementario",
      "Introducción a R y RStudio"
    ]
  },
  {
    "objectID": "intro_R.html#paquetes",
    "href": "intro_R.html#paquetes",
    "title": "Introducción a R y RStudio",
    "section": "Paquetes",
    "text": "Paquetes\nR se compone de un sistema base y de paquetes (librerías) que amplían sus funcionalidades. Un paquete es una colección de funciones, datos y documentación que extiende las capacidades del lenguaje para tareas específicas.\nExisten distintos tipos de paquetes:\n\nBase: se instalan y activan junto con R.\nRecomendados: también se instalan por defecto, pero requieren ser cargados manualmente.\nAdicionales: más de 17.000 disponibles en el repositorio oficial CRAN, listos para ser instalados según necesidad. Además, algunos paquetes pueden descargarse desde otros repositorios como GitHub y Bioconductor.\n\nAl ser open source, cualquier persona puede desarrollar y publicar nuevos paquetes. Esto convierte a R en una herramienta en constante evolución.\n\nInstalación\nLos paquetes pueden instalarse desde R o RStudio o (si no hay acceso a internet o trabajamos con conexiones de uso medido) desde archivos locales .zip o .tar.gz, descargados previamente desde CRAN u otros repositorios.\nEn RStudio, los paquetes se gestionan desde la pestaña Packages (bloque inferior derecho). Para instalar uno nuevo:\n\nHacer clic en , se abrirá una ventana emergente:\n\n\n\n\n\n\n\nEspecificar el nombre del paquete a instalar.\nMarcar la opción Install dependencies para incluir automáticamente sus dependencias.\nAl presionar el botón Install, R internamente traduce esta acción a la función install.packages().\n\nLos paquetes deben instalarse una única vez por computadora cuando se los va a utilizar por primera vez. A partir de entonces, sólo es necesario cargarlos al inicio de cada sesión mediante la función library():\n\nlibrary(nombre_del_paquete)\n\n\n\nDependencias\nMuchos paquetes requieren funciones de otros paquetes para funcionar. Estos paquetes (dependencias) deben estar instaladas previamente, de lo contrario la ejecución de una función puede fallar por no encontrar otra interna. Por eso, es recomendable dejar seleccionada la opción Install dependencies al instalar.\n\n\n\n\n\n\nPaquetes a instalar\n\n\n\nPara trabajar durante el curso, deberemos instalar los siguientes paquetes y sus dependencias:\n\n# Manejo de datos\ninstall.packages(\"tidyverse\", dependencies = T)\n\ninstall.packages(\"janitor\", dependencies = T)\n\n# Modelos de meta-análisis\ninstall.packages(\"metafor\", dependencies = T)\n\ninstall.packages(\"meta\", dependencies = T)\n\n# Paletas aptas para daltonismo\ninstall.packages(\"scico\", dependencies = T)\n\n# Visualización avanzada\nremotes::install_github(\"daniel1noble/orchaRd\")",
    "crumbs": [
      "Material suplementario",
      "Introducción a R y RStudio"
    ]
  },
  {
    "objectID": "intro_R.html#objetos",
    "href": "intro_R.html#objetos",
    "title": "Introducción a R y RStudio",
    "section": "Objetos",
    "text": "Objetos\nEn R, los datos, resultados, funciones y estructuras se almacenan en objetos, que constituyen la unidad fundamental de trabajo en el lenguaje.\nPara crear un objeto, se utiliza el operador de asignación &lt;- (también se acepta = aunque no se recomienda) para asignar un valor a un nombre:\n\nx &lt;- 10 \n\nEn este ejemplo, el número 10 se asigna al objeto llamado x. A partir de ese momento, podemos utilizar x en otras operaciones:\n\nx + 5  # devuelve 15\n\nLos nombres de objetos:\n\nDeben comenzar con una letra y pueden incluir letras, números, puntos (.) y guiones bajos (_).\nNo deben coincidir con palabras reservadas a funciones del lenguaje.\nSon sensibles a mayúsculas/minúsculas: Edad y edad son objetos distintos.\n\nLos objetos contenedores de datos más simples pertenecen a cinco clases que se denominan atómicas y que son los siguientes tipos de datos:\n\ninteger: números enteros.\nnumeric: números reales (también llamados “doble precisión”).\ncomplex: números complejos.\ncharacter: cadenas de texto o caracteres.\nlogical: valores lógicos (TRUE o FALSE).\n\nnúmero &lt;- 25           # entero\ndecimal &lt;- 3.14        # numérico\ntexto &lt;- \"Hola\"        # carácter\nlogico &lt;- TRUE         # lógico (booleano)\n\n\nAdemás de los tipos atómicos, los datos pueden organizarse en estructuras contenedoras que permiten agrupar múltiples valores:\n\nVector: conjunto de elementos del mismo tipo, ordenados linealmente. Se construye con la función c().\nLista: colección ordenada de objetos de distinto tipo o longitud, creada con list().\nDataframe: estructura bidimensional donde cada columna es un vector del mismo largo (generalmente del mismo tipo). Se construye con data.frame() o, en el tidyverse, con tibble().\n\n# Vector\nvector  &lt;- c(1, 2, 3, 4)\n\n# Lista\nlista &lt;- list(vector, \"elemento_2\") # lista\n\n# Dataframe (R base)\ndataframe &lt;- data.frame(\n  var1 = vector,\n  var2 = vector + 5,\n  var3 = vector * vector^2\n)\n\n# Dataframe (tidyverse)\ntibble &lt;- tibble(\n  var1 = vector,\n  var2 = vector + 5,\n  var3 = vector * vector^2\n)",
    "crumbs": [
      "Material suplementario",
      "Introducción a R y RStudio"
    ]
  },
  {
    "objectID": "intro_R.html#archivos-de-datos",
    "href": "intro_R.html#archivos-de-datos",
    "title": "Introducción a R y RStudio",
    "section": "Archivos de datos",
    "text": "Archivos de datos\nR permite importar tablas de datos desde diversos formatos, tanto utilizando funciones de R base como funciones provistas por paquetes específicos.\nEl formato más común es el texto plano (ASCII), donde los valores están organizados en columnas separadas por caracteres delimitadores. Los separadores más habituales incluyen:\n\nComa (,)\nPunto y coma (;)\nTabulación (\\t)\nBarra vertical (|)\n\nEstos archivos suelen tener una cabecera (header) en la primera fila con los nombres de las variables, y cada columna debe contener datos del mismo tipo (números, texto, lógicos, etc.).\nPara importar correctamente un archivo es importante conocer su estructura:\n\nSi incluye o no cabecera.\nQué carácter se usa como separador.\nEl tipo de codificación (UTF-8, Latin1, etc.).\n\nDado que son archivos de texto, pueden visualizarse con editores simples como el Bloc de Notas o desde RStudio, lo que facilita su inspección previa.\nPara cargar los datos desde un archivo de texto plano o una hoja de cálculo de Excel usamos el código:\n\ndatos &lt;- read.xxx(\"mis_datos.txt\")\n\n(Se debe reemplazar read.xxx() por la función correspondiente: read.table(), read.csv(), read_delim(), read_excel(), etc., según el caso).\nR también permite cargar bases de datos incluidas en paquetes instalados mediante:\n\ndata(nombre_datos)\n\ndatos &lt;- nombre_datos",
    "crumbs": [
      "Material suplementario",
      "Introducción a R y RStudio"
    ]
  },
  {
    "objectID": "intro_R.html#buenas-prácticas",
    "href": "intro_R.html#buenas-prácticas",
    "title": "Introducción a R y RStudio",
    "section": "Buenas prácticas",
    "text": "Buenas prácticas\nAdoptar buenas prácticas desde el inicio mejora la reproducibilidad, facilita el trabajo colaborativo y reduce errores. Algunas recomendaciones clave son:\n\nTrabajar siempre dentro de un proyecto de RStudio (.Rproj). Esto permite organizar los archivos, mantener rutas relativas consistentes y acceder a funcionalidades específicas como control de versiones o panel de archivos integrados.\nIncluir al comienzo de cada script las líneas de activación de paquetes necesarios, utilizando la función library().\n\n\n\nCargar los datos una vez activados los paquetes, para garantizar que todas las funciones requeridas estén disponibles.\nDocumentar el código mediante comentarios iniciados con #. Esto permite entender qué hace cada bloque de código, facilitando futuras modificaciones o revisiones.\nUsar espacios e indentación adecuada para mejorar la legibilidad. Esto es especialmente importante en estructuras anidadas (como condicionales, bucles o funciones).\n\nUna guía de estilo ampliamente recomendada —aunque no oficial— es la de tidyverse. Incluye ejemplos concretos de buenas y malas prácticas para nombrar variables, manejar líneas largas, usar sangrías, entre otros aspectos. Puede consultarse en: https://style.tidyverse.org/\n\n\n\n\n\n\nImportante\n\n\n\nEste apunte ofrece un resumen general para quienes deseen repasar los aspectos básicos de R y RStudio.\n\nSi no cuentan con experiencia previa en R y necesitan una introducción más detallada, podés consultar los siguientes recursos:\n\nCurso de Epidemiología Nivel Avanzado - Unidad 1: Introducción a R.\nEpiR Handbook – secciones Aspectos básicos y Gestión de datos.\n\nAnte cualquier duda específica, recuerden que pueden comunicarse con los/as docentes del curso.",
    "crumbs": [
      "Material suplementario",
      "Introducción a R y RStudio"
    ]
  },
  {
    "objectID": "tidyverse.html",
    "href": "tidyverse.html",
    "title": "Introducción a tidyverse",
    "section": "",
    "text": "Este material es parte del curso interno de Introducción a la Revisión Sistemática con Meta-análisis  © 2025 Instituto Nacional de Epidemiología “Dr. Juan H. Jara” (ANLIS) -  CC BY-NC 4.0"
  },
  {
    "objectID": "tidyverse.html#introducción",
    "href": "tidyverse.html#introducción",
    "title": "Introducción a tidyverse",
    "section": "Introducción",
    "text": "Introducción\nTidyverse es el nombre que se ha dado al conjunto de paquetes desarrollados o apoyados por Hadley Wickham (jefe científico de Posit/RStudio) y su equipo, para ciencia de datos con R.\nTodos estos paquetes están diseñados para funcionar juntos y comparten una misma filosofía, que se puede consultar en The tidy tools manifesto.\nLos cuatro principios básicos en los que se basa son:\n\nReutilizar las estructuras de datos\nResolver problemas complejos combinando varias piezas sencillas\nUtilizar programación funcional\nDiseño para humanos\n\nLos paquetes incluidos en el tidyverse tienen como objetivo cubrir todas las fases del análisis de datos dentro de R: importar datos, ponerlos en formato ordenado (tidy data), buscar relaciones entre ellos (mediante su transformación, visualización y creación de modelos) y comunicar los resultados.\nLa palabra “tidy” se puede traducir como “ordenado” y refiere a que los datos deben cumplir con una estructura determinada donde:\n\nCada variable es una columna de la tabla de datos.\nCada observación es una fila de la tabla de datos.\nCada tabla responde a una unidad de observación o análisis.\n\n\n\n\n\n\nAdemás de los paquetes principales que realizan estas funciones, al instalar el tidyverse también se proporcionan otros que ayudan a trabajar con fechas, cadenas de caracteres o factores siguiendo los mismos principios.\nUna de las interesantes incorporaciones transversales en el ambiente tidyverse es el uso de tuberías (del inglés pipe).\nUna tubería conecta un trozo de código con otro mediante el conector |&gt; que surge del paquete magrittr que permite transformar llamadas de funciones anidadas (con muchos paréntesis) en una simple serie de operaciones que son más fáciles de escribir y comprender. A partir de la versión 4.1.0 de R existe una tubería similar en su versión nativa (|&gt;). El uso de cualquiera de ellas es similar.\nResponde al principio donde cada función es un paso y la forma de trabajar se puede ver en el siguiente esquema general:\n\n\n\n\n\nMostramos el funcionamiento básico comparativo en la siguiente porción de código:\n\n# Utilizamos para la demostración el dataset mtcars incorporado en R (datos sobre autos)\n\nhead(sqrt(mtcars)) \n\n                       mpg      cyl     disp        hp     drat       wt\nMazda RX4         4.582576 2.449490 12.64911 10.488088 1.974842 1.618641\nMazda RX4 Wag     4.582576 2.449490 12.64911 10.488088 1.974842 1.695582\nDatsun 710        4.774935 2.000000 10.39230  9.643651 1.962142 1.523155\nHornet 4 Drive    4.626013 2.449490 16.06238 10.488088 1.754993 1.793042\nHornet Sportabout 4.324350 2.828427 18.97367 13.228757 1.774824 1.854724\nValiant           4.254409 2.449490 15.00000 10.246951 1.661325 1.860108\n                      qsec vs am     gear     carb\nMazda RX4         4.057093  0  1 2.000000 2.000000\nMazda RX4 Wag     4.125530  0  1 2.000000 2.000000\nDatsun 710        4.313931  1  1 2.000000 1.000000\nHornet 4 Drive    4.409082  1  0 1.732051 1.000000\nHornet Sportabout 4.125530  0  0 1.732051 1.414214\nValiant           4.496665  1  0 1.732051 1.000000\n\n# en la línea de código de arriba estamos pidiendo mostrar la cabecera (6 primeras \n# observaciones de la tabla de datos) de la raíz cuadrada de los valores de la tabla mtcars, \n# en formato del lenguaje clásico (anidado)\n\n# Ahora activamos maggritr (viene incorporado con tidyverse)\n\nlibrary(magrittr) \n\n# ejecutamos la línea anterior convertida al formato tubería\n\nmtcars |&gt;\n  sqrt() |&gt;\n  head()\n\n                       mpg      cyl     disp        hp     drat       wt\nMazda RX4         4.582576 2.449490 12.64911 10.488088 1.974842 1.618641\nMazda RX4 Wag     4.582576 2.449490 12.64911 10.488088 1.974842 1.695582\nDatsun 710        4.774935 2.000000 10.39230  9.643651 1.962142 1.523155\nHornet 4 Drive    4.626013 2.449490 16.06238 10.488088 1.754993 1.793042\nHornet Sportabout 4.324350 2.828427 18.97367 13.228757 1.774824 1.854724\nValiant           4.254409 2.449490 15.00000 10.246951 1.661325 1.860108\n                      qsec vs am     gear     carb\nMazda RX4         4.057093  0  1 2.000000 2.000000\nMazda RX4 Wag     4.125530  0  1 2.000000 2.000000\nDatsun 710        4.313931  1  1 2.000000 1.000000\nHornet 4 Drive    4.409082  1  0 1.732051 1.000000\nHornet Sportabout 4.125530  0  0 1.732051 1.414214\nValiant           4.496665  1  0 1.732051 1.000000\n\n# lo mismo con la tubería nativa de R (no hay que activar ningún paquete)\n\nmtcars |&gt; \n  sqrt() |&gt; \n  head()\n\n                       mpg      cyl     disp        hp     drat       wt\nMazda RX4         4.582576 2.449490 12.64911 10.488088 1.974842 1.618641\nMazda RX4 Wag     4.582576 2.449490 12.64911 10.488088 1.974842 1.695582\nDatsun 710        4.774935 2.000000 10.39230  9.643651 1.962142 1.523155\nHornet 4 Drive    4.626013 2.449490 16.06238 10.488088 1.754993 1.793042\nHornet Sportabout 4.324350 2.828427 18.97367 13.228757 1.774824 1.854724\nValiant           4.254409 2.449490 15.00000 10.246951 1.661325 1.860108\n                      qsec vs am     gear     carb\nMazda RX4         4.057093  0  1 2.000000 2.000000\nMazda RX4 Wag     4.125530  0  1 2.000000 2.000000\nDatsun 710        4.313931  1  1 2.000000 1.000000\nHornet 4 Drive    4.409082  1  0 1.732051 1.000000\nHornet Sportabout 4.125530  0  0 1.732051 1.414214\nValiant           4.496665  1  0 1.732051 1.000000\n\n# la tubería le da mucha mas claridad al código separandolo en partes, como \n# si fuesen oraciones de un párrafo\n\nBase gramatical\nLa intención de los desarrolladores para este conjunto de paquetes es lograr incorporar una gramática a la sintaxis de las funciones y sus argumentos buscando un entendimiento semántico más claro.\nUna prueba de ello, es que la mayoría de las funciones son verbos (en ingles) que se entrelazan con objetos y argumentos que permiten construir “frases”. Ejemplo de ello se muestra en paquetes como dplyr."
  },
  {
    "objectID": "tidyverse.html#paquete-tidyverse",
    "href": "tidyverse.html#paquete-tidyverse",
    "title": "Introducción a tidyverse",
    "section": "Paquete tidyverse",
    "text": "Paquete tidyverse\nEl paquete tidyverse base actual (versión 2.0.0) se puede descargar del repositorio oficial CRAN mediante menú Packages de RStudio o ejecutando:\n\ninstall.packages(\"tidyverse\")\n\nSe activa mediante:\n\nlibrary(tidyverse)\n\nobservamos que nos informa sobre la versión del paquete, el listado de paquetes que acabamos de activar sólo llamando a tidyverse y una serie de conflictos de nombres de funciones.\nEsto es muy habitual cuando activamos varios paquetes, dado que las funciones que se encuentran dentro de ellos pueden llamarse iguales.\nPor ejemplo, existe en el paquete base stats y en el paquete dplyr (que es parte de tidyverse) una función llamada filter(), por lo tanto al activar tidyverse nos informa de esta manera: dplyr::filter() masks stats::filter()\nEn este caso, cuando necesitemos asegurarnos que la función que deseamos ejecutar pertenece a determinado paquete, es recomendable escribirla de la siguiente forma:\n\nnombre_paquete::nombre_función\n\nstats:filter() para la función filter() del paquete stats\ndplyr:filter() para la función filter() del paquete dplyr\nLos paquetes incluidos que se instalan en esta versión son:\n\ntidyverse_packages()\n\n [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n[21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n[25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n[29] \"tidyr\"         \"xml2\"          \"tidyverse\"    \n\n\nExisten otros paquetes (la cantidad crece mes a mes) que son creados bajo la misma filosofía pero no están incluidos. En esos casos hay que instalarlos y activarlos individualmente.\nPara profundizar sobre tidyverse, se puede visitar el sitio https://www.tidyverse.org/ y el libro traducido al español r4ds o la nueva versión r4ds 2e, por ahora solo en inglés."
  },
  {
    "objectID": "tidyverse.html#lectura-y-escritura-de-datos",
    "href": "tidyverse.html#lectura-y-escritura-de-datos",
    "title": "Introducción a tidyverse",
    "section": "Lectura y escritura de datos",
    "text": "Lectura y escritura de datos\n\nPaquete readr\nreadr contiene funciones similares a las de la familia read.table() de Rbase pero desarrollados bajo el ecosistema tidyverse.\nLos archivos de texto plano (ASCII y otras codificaciones) son universalmente utilizados por la mayoría de los gestores de bases de datos y/o planillas de cálculo. Generalmente encontrados con extensiones .txt o .csv (por comma-separated values) son el tipo de archivo de datos más habitual dentro del lenguaje R.\nEstos datos planos tienen dos peculiaridades:\n\nLa cabecera (en inglés header)\nEl caracter o símbolo separador que indica la separación de columnas: pueden estar separadas por comas, puntos y comas, por tabulación, etc…\n\nLa cabecera puede existir o no, y la inclusión o no de la cabecera se maneja desde los argumentos col_names y skip.\nCon col_names = TRUE incluimos la primer fila como cabecera (nombre de las columnas) y en FALSE la salteamos.\nCon skip = 0 la lectura de produce desde la primer fila (se puede omitir), pero si la cabecera fuese compleja con varias filas entre títulos y subtítulos, debemos indicar cuantas filas iniciales se “saltea”. Por ejemplo con skip = 5 se saltea las primeras 5 filas del archivo.\nEl otro elemento a tener en cuenta es el caracter separador que utiliza el archivo para indicar cuando comienza una nueva columna (variable).\nGeneralmente los separadores más comunes son: la coma (,), el punto y coma (;), el tabulador (TAB), el espacio ( ), el caracter pipe (|), entre otros posibles.\nAlgunas de las funciones del paquete asumen un separador particular. Por caso read_csv() lee separados por coma y read_tsv() separado por tabulaciones, pero la función read_delim() permite que definamos el separador a través del argumento delim.\nEn forma detallada el paquete readr soporta siete formatos de archivo a partir de siete funciones:\n\nread_csv(): archivos separados por comas (CSV)\nread_tsv(): archivos separados por tabulaciones\nread_delim(): archivos separados con delimitadores generales\nread_fwf(): archivos con columnas de ancho fijo\nread_table(): archivos formato tabla con columnas separadas por espacios\nread_log(): archivos log web\n\nEn comparación con las funciones base de R, las funciones de readr:\n\nUsan un esquema de nombres consistente de parámetros\nSon más rápidas.\nAnalizan eficientemente los formatos de datos comunes, incluyendo fecha/hora.\nMuestran una barra de progreso si la carga va a llevar un tiempo. (para archivos grandes)\n\nViene incluida dentro de la instalación de tidyverse y se activa con él, pero también permite activarse solo:\n\nlibrary(readr)\n\nAlgunos ejemplos de sintaxis:\n\nLeemos un archivo sin cabecera separado por comas bajo el nombre datos:\n\n\n\n\n\n\n\n9,Leone,Fernando,M,1958-12-24\n26,Garcia,Laura,M,1954-01-21\n35,Salamone,Nicolas,M,1993-06-27\n48,Gonzalez,Viviana,F,1965-06-21\n\n\n\n\ndatos &lt;- read_csv(\"datos/ejemplo-datos.csv\", \n                  col_names = F)\n\n\nLeemos el mismo archivo con cabecera y separado por punto y comas, bajo el nombre info\n\n\n\n\n\n\n\nIden;Apellido;Nombre;sexo;FNac\n9;Leone;Fernando;M;1958-12-24\n26;Garcia;Laura;M;1954-01-21\n35;Salamone;Nicolas;M;1993-06-27\n48;Gonzalez;Viviana;F;1965-06-21\n\n\n\n\ninfo &lt;- read_csv2(\"datos/ejemplo-datos-header.csv\", col_names = T)\n\nObservemos que dependiendo de la forma en que se encuentren los datos en el archivo (distintos separadores, cabeceras, etc), aplicamos la función correspondiente. Además cada vez que hacemos una lectura la función se encarga de analizar (parse) el tipo de dato que hay en cada columna.\n\ndatos\n\n# A tibble: 4 × 5\n     X1 X2       X3       X4    X5        \n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;date&gt;    \n1     9 Leone    Fernando M     1958-12-24\n2    26 Garcia   Esteban  M     1954-01-21\n3    35 Salamone Nicolas  M     1993-06-27\n4    48 Gonzalez Viviana  F     1965-06-21\n\n\nEn el primer ejemplo, el archivo no tenía cabecera y por lo tanto, al importarse los datos cada variable se denominó X1, X2, X3, etc.\nOtra forma de mostrar la estructura del archivo y sus columnas es con la función glimpse():\n\nglimpse(info)\n\nRows: 4\nColumns: 5\n$ Iden     &lt;dbl&gt; 9, 26, 35, 48\n$ Apellido &lt;chr&gt; \"Leone\", \"Garcia\", \"Salamone\", \"Gonzalez\"\n$ Nombre   &lt;chr&gt; \"Fernando\", \"Laura\", \"Nicolas\", \"Viviana\"\n$ Sexo     &lt;chr&gt; \"M\", \"M\", \"M\", \"F\"\n$ FNac     &lt;date&gt; 1958-12-24, 1954-01-21, 1993-06-27, 1965-06-21\n\n\nLos posibles tipos de datos son los atómicos del lenguaje más algún agregado: character, integer, numeric, double, logical y date/time.\nPor ejemplo, en la tabla leída anteriormente las columnas donde hay números enteros fueron reconocidos como double (&lt;dbl&gt;), los que tienen algún caracter como character (&lt;chr&gt;) y las fechas como date (&lt;date&gt;).\nAgregamos unos argumentos más y ejemplificamos la sintaxis con read_delim() para leer un archivo con cabecera compleja (la tabla comienza en la fila 9) separado por caracteres | (pipes).\n\nread_delim(\"ejemplo-datos-header-skip.txt\", \n           col_names = T, \n           skip = 8, \n           delim = \"|\")\n\n\nCuando realicemos lecturas no debemos olvidar asignar su salida a un nombre, que será el nombre del dataframe que reciba los datos dentro de nuestra sesión de trabajo. (&lt;-)\n\n\nFunciones de escritura\nDentro del paquete coexisten funciones espejo de escritura para las posibilidades de lectura más relevantes. Así encontramos estos cuatro:\n\nwrite_csv(): escribe archivos separados por comas (csv)\nwrite_csv2(): escribe archivos separados por punto y comas (csv)\nwrite_tsv(): escribe archivos separados por tabulaciones\nwrite_delim(): escribe archivos separados con delimitadores definidos por el usuario\n\nLos argumentos son generales y para el caso del último más extensos, dado que hay que definir cual es el separador que deseamos en el archivo.\n\nargs(write_delim)\n\nfunction (x, file, delim = \" \", na = \"NA\", append = FALSE, col_names = !append, \n    quote = c(\"needed\", \"all\", \"none\"), escape = c(\"double\", \n        \"backslash\", \"none\"), eol = \"\\n\", num_threads = readr_threads(), \n    progress = show_progress(), path = deprecated(), quote_escape = deprecated()) \nNULL\n\n\nPor ejemplo para exportar un conjunto de datos en texto plano al que denominaremos “ejemplo.csv” con separador punto y coma y cabecera incluida podemos hacer:\n\nwrite_delim(x = datos, file = \"ejemplo.csv\", delim = \";\")\n\no más sencillo:\n\nwrite_csv2(datos, \"ejemplo.csv\") # define cabecera y separador ;\n\n\n\n\nPaquete readxl\nUno de los formatos de documentos más comunes en los que se almacenan datos son las hojas de cálculo, en particular, las creadas con el programa Excel de Microsoft Office.\nEl paquete readxl es parte del ecosistema tidyverse y permite leer este tipo de archivos.\nPosee compatibilidad con hojas de cálculo de Excel 97-03, de extensión .xls, y con hojas de cálculo de las versiones más recientes de Excel, de extensión, .xlsx\nLa primera función interesante es excel_sheets(), útil para conocer y listar los nombre de las hojas contenidas dentro de un archivo (libro) Excel.\nPor ejemplo, supongamos que tenemos un archivo denominado “datos.xlsx” y queremos saber por cuantas hojas está compuesto y que nombre tienen.\n\nlibrary(readxl) # hay que activarlo independientemente de tidyverse\n\nexcel_sheets(\"datos/datos.xlsx\")\n\n[1] \"diabetes\"   \"vigilancia\" \"mortalidad\"\n\n\nObtenemos de esta manera información sobre el archivo. Hay tres hojas llamadas diabetes, vigilancia y mortalidad.\nPara poder leer cada una de estas hojas de datos debemos usar la función read_excel(), que tiene los siguientes argumentos:\n\nargs(read_excel)\n\nfunction (path, sheet = NULL, range = NULL, col_names = TRUE, \n    col_types = NULL, na = \"\", trim_ws = TRUE, skip = 0, n_max = Inf, \n    guess_max = min(1000, n_max), progress = readxl_progress(), \n    .name_repair = \"unique\") \nNULL\n\n\nDonde los más relevantes son:\npath: nombre del archivo y la ubicación (si fuese necesaria) entre comillas\nsheet: nombre de la hoja o número de ubicación\ncol_names: si se activa toma la primer fila como nombres de columnas (variables)\nskip: permite saltear una cantidad determinada de filas antes de comenzar la lectura\nEn primer lugar, cuando ejecutamos esta función, llama a otra denominada excel_format() que determina frente a que formato de archivo estamos. Si es un Excel tipo .xsl o tipo .xlsx. En relación a esta respuesta, luego aplica la función específica para cada caso - read_xls() o readxlsx().\nTodas estas funciones mencionadas en el procedimiento que sigue read_excel() se pueden utilizar en forma específica.\nContinuemos con el archivo “datos.xlsx” y procedamos a leer los datos de su primer hoja, llamada diabetes.\n\ndiabetes &lt;- read_excel(path = \"datos/datos.xlsx\", \n                       sheet = \"diabetes\",\n                       col_names = T)\n\nhead(diabetes) # mostramos las 6 primeras observaciones\n\n# A tibble: 6 × 8\n    A1C  hba1 GLUCB   SOG Tol_Glucosa    DM    SM  HOMA\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  6.17   7.9   101   122 IFG             0     1  4.04\n2  5.58   7.2   103   100 IFG             0     0  5.03\n3  5.38   7.1   103    90 IFG             0     1  2.92\n4  5.38   6.6   109    96 IFG             0     1  4.79\n5  5.19   6.3   107    69 IFG             0     1  3.06\n6  4.89   6      NA   117 IFG             0     0  5.77\n\n\nObservemos que en los argumentos escribimos el nombre del archivo que se encuentra en nuestro proyecto y por lo tanto en nuestra carpeta activa, el nombre de la hoja y nos aseguramos que la primer fila representa a la cabecera de la tabla (sus nombres de variables).\nComo el paquete readxl se inscribe dentro del universo tidyverse el formato de salida es un dataframe/tibble. En este caso de 23 observaciones por 8 variables.\nAhora leamos la segunda hoja de nombre vigilancia.\n\nvigilancia &lt;- read_excel(path = \"datos/datos.xlsx\", \n                         sheet = 2, \n                         col_names = F)\n\nhead(vigilancia) # mostramos las 6 primeras observaciones\n\n# A tibble: 6 × 9\n   ...1 ...2        ...3      ...4  ...5  ...6  ...7 ...8  ...9                 \n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                \n1   875 09/28/2015  2015 544080000     1    31     1 F     VIGILANCIA EN SALUD …\n2   875 42317       2015 544080000     1    35     1 F     VIGILANCIA EN SALUD …\n3   875 42317       2015 544080000     1    47     1 F     VIGILANCIA EN SALUD …\n4   307 09/26/2015  2015 544005273     1    23     1 M     VIGILANCIA INTEGRADA…\n5   307 09/24/2015  2015 544005273     1    19     1 M     VIGILANCIA INTEGRADA…\n6   875 09/28/2015  2015 544080000     1    63     1 F     VIGILANCIA EN SALUD …\n\n\nCentremos nuestra mirada en los argumentos anteriores: en lugar del nombre de la hoja usamos un 2 que es su ubicación (la segunda hoja del archivo Excel) y configuramos a col_names con F (false) porque el conjunto de datos no tiene cabecera.\nCuando ocurre esta situación donde la tabla no tiene nombre de columnas readxl le asigna nombres del tipo ...1, ...2, ...x\nFinalmente leemos la última hoja disponible del archivo.\n\nmortalidad &lt;- read_excel(path = \"datos/datos.xlsx\", \n                         sheet = \"mortalidad\",\n                         col_names = T, \n                         skip = 1)\n\nhead(mortalidad) # mostramos las 6 primeras observaciones\n\n# A tibble: 5 × 10\n  grupo_edad grupo.I.1.1 grupo.II.1.1 grupo.III.1.1 grupo.I.2.1 grupo.II.2.1\n  &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 30-44               41          202           222         539         1438\n2 45-59               99         1071           181         759         6210\n3 60-69              114         1782           119         985         9238\n4 70-79              221         2336           119        1571        12369\n5 80+                362         2492            81        2523        14642\n# ℹ 4 more variables: grupo.III.2.1 &lt;dbl&gt;, grupo.I.3.1 &lt;dbl&gt;,\n#   grupo.II.3.1 &lt;dbl&gt;, grupo.III.3.1 &lt;dbl&gt;\n\n\nLo novedoso de esta lectura es el argumento skip = 1 que debimos incorporar dado que, en este caso, la hoja de Excel comienza con una línea de título que no pertenece al conjunto de datos. También que el argumento sheet permite el nombre de la hoja elegida entre comillas.\nRetomando los argumentos generales de la función podemos mencionar estos otros:\nn_max: número máximo de filas leídas\nrange: rango de celdas a leer (definidas como se suele usar en Excel, por ej: B3:D87)\ncol_types: especificación del tipo de dato para cada columna leída. Se pueden utilizar los tipos habituales “numeric”, “logical”, “text”, “date”, etc. Existen dos tipos específicos más: “skip” que saltea la lectura de la columna y “guess” que permite que la función decida cual es el formato adecuado de importación. Este último es el modo predeterminado cuando no especificamos el argumento.\nna: caracter o vector que deseamos se interprete como valor perdido (missing). Por defecto las celdas vacías se interpretan de esta forma y se le asigna NA"
  },
  {
    "objectID": "tidyverse.html#gestión-de-datos-con-el-paquete-dplyr",
    "href": "tidyverse.html#gestión-de-datos-con-el-paquete-dplyr",
    "title": "Introducción a tidyverse",
    "section": "Gestión de datos con el Paquete dplyr",
    "text": "Gestión de datos con el Paquete dplyr\nEl paquete dplyr es parte del universo tidyverse que fue desarrollado por Hadley Wickham a partir de optimizar una versión del paquete plyr.\nLa contribución significativa del paquete es proporcionar una “gramática” (funciones-verbos) para la manipulación y operaciones de datos que lo hace más fácil de entender.\nLas funciones clave del paquete, responden a las siguientes acciones (verbos):\n\nselect(): devuelve un conjunto de columnas (variables)\nrename(): renombra variables en una conjunto de datos\nfilter(): devuelve un conjunto de filas (observaciones) según una o varias condiciones lógicas\narrange(): reordena filas de un conjunto de datos\nmutate(): añade nuevas variables/columnas o transforma variables existentes\nsummarise()/summarize(): genera resúmenes estadísticos de diferentes variables en el conjunto de datos.\ngroup_by(): agrupa un conjunto de filas seleccionado, en un conjunto de filas de resumen de acuerdo con los valores de una o más columnas o expresiones.\ncount(): contabiliza valores que se repiten, es decir genera tabla de frecuencias.\n\nAdemás como todos los paquetes del tidyverse integra al operador %&gt;% (pipe) logrando una única tubería (“pipeline”).\n\nArgumentos comúnes en las funciones dplyr\nTodas las funciones, básicamente, tienen en común una serie de argumentos.\n\nEl primer argumento es el nombre del conjunto de datos (objeto donde esta nuestra tabla de datos).\nLos otros argumentos describen que hacer con el conjunto de datos especificado en el primer argumento, podemos referirnos a las columnas en el objeto directamente sin utilizar el operador $, es decir sólo con el nombre de la columna/variable.\nEl valor de retorno es un nuevo conjunto de datos.\nLos conjuntos de datos deben estar bien organizados/estructurados, es decir debe existir una observación por columna y, cada columna representar una variable, medida o característica de esa observación. Es decir, debe cumplir con tidy data.\n\n\n\nActivación del paquete\ndplyr está incluído en el paquete tidyverse, por lo que se encuentra disponible si tenemos activado a este último.\nTambién se puede activar en forma independiente, aunque no es necesario si ya activamos tidyverse:\n\nlibrary(dplyr)\n\n\n\nConjunto de datos para ejemplo\nVisualizar y entender el funcionamiento de estos “verbos” de manipulación es posible si ejemplificamos su aplicación. Por este motivo vamos a leer un conjunto de datos que servirá para ejercitar las funciones del paquete.\n\ndatos &lt;- read_csv(\"datos/noti-vih.csv\") # asignamos la lectura a datos\n\nhead(datos) # mostramos las 6 primeras observaciones\n\n# A tibble: 6 × 4\n  jurisdiccion   año casos      pob\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 Buenos Aires  2015  1513 16626374\n2 Buenos Aires  2016   957 16789474\n3 CABA          2015   901  3054237\n4 CABA          2016   427  3050000\n5 Catamarca     2015    69   396552\n6 Catamarca     2016    51   401575\n\n\nLa tabla de datos “noti-vih.csv” contiene datos de notificación de vih por jurisdicción de Argentina para los años 2015 y 2016.\n\n\nFunción select()\nEsta función selecciona las variables que especificamos devolviendo un conjunto de datos “recortado por columna”.\nVeamos algunas variaciones de ayuda para hacer estas selecciones.\n\nTodas las variables menos pob:\n\ndatos |&gt; \n  select(-pob)\n\n# A tibble: 48 × 3\n   jurisdiccion   año casos\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 Buenos Aires  2015  1513\n 2 Buenos Aires  2016   957\n 3 CABA          2015   901\n 4 CABA          2016   427\n 5 Catamarca     2015    69\n 6 Catamarca     2016    51\n 7 Chaco         2015    15\n 8 Chaco         2016     9\n 9 Chubut        2015   110\n10 Chubut        2016    89\n# ℹ 38 more rows\n\n\n\n\n\nOtra forma para el mismo resultado anterior (mediante el operador rango :):\n\ndatos |&gt; \n  select(jurisdiccion:casos)\n\n# A tibble: 48 × 3\n   jurisdiccion   año casos\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 Buenos Aires  2015  1513\n 2 Buenos Aires  2016   957\n 3 CABA          2015   901\n 4 CABA          2016   427\n 5 Catamarca     2015    69\n 6 Catamarca     2016    51\n 7 Chaco         2015    15\n 8 Chaco         2016     9\n 9 Chubut        2015   110\n10 Chubut        2016    89\n# ℹ 38 more rows\n\n\nLas variables jurisdiccion y casos:\n\ndatos |&gt; \n  select(jurisdiccion, casos)\n\n# A tibble: 48 × 2\n   jurisdiccion casos\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Buenos Aires  1513\n 2 Buenos Aires   957\n 3 CABA           901\n 4 CABA           427\n 5 Catamarca       69\n 6 Catamarca       51\n 7 Chaco           15\n 8 Chaco            9\n 9 Chubut         110\n10 Chubut          89\n# ℹ 38 more rows\n\n\nOtra forma para el mismo resultado anterior (mediante números de columna):\n\ndatos |&gt; \n  select(1, 3)\n\n# A tibble: 48 × 2\n   jurisdiccion casos\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Buenos Aires  1513\n 2 Buenos Aires   957\n 3 CABA           901\n 4 CABA           427\n 5 Catamarca       69\n 6 Catamarca       51\n 7 Chaco           15\n 8 Chaco            9\n 9 Chubut         110\n10 Chubut          89\n# ℹ 38 more rows\n\n\nTodas las variables pasando año a la primera columna:\n\ndatos |&gt; \n  select(\"año\", everything())\n\n# A tibble: 48 × 4\n     año jurisdiccion casos      pob\n   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  2015 Buenos Aires  1513 16626374\n 2  2016 Buenos Aires   957 16789474\n 3  2015 CABA           901  3054237\n 4  2016 CABA           427  3050000\n 5  2015 Catamarca       69   396552\n 6  2016 Catamarca       51   401575\n 7  2015 Chaco           15  1153846\n 8  2016 Chaco            9  1125000\n 9  2015 Chubut         110   567010\n10  2016 Chubut          89   577922\n# ℹ 38 more rows\n\n\n\nOtros posibles argumentos son:\n\nstarts_with(): selecciona todas las columnas que comiencen con el patrón indicado.\nends_with(): selecciona todas las columnas que terminen con el patrón indicado.\ncontains(): selecciona las columnas que posean el patrón indicado.\nmatches(): similar a contains(), pero permite poner una expresión regular.\nall_of(): selecciona las variables pasadas en un vector (todos los nombres deben estar presentes o devuelve un error)\nany_of(): idem anterior excepto que no se genera ningún error para los nombres que no existen.\nnum_range(): selecciona variables con nombre combinados con caracteres y números (ejemplo: num_range(“x”, 1:3) selecciona las variables x1, x2 y x3.\nwhere(): aplica una función a todas las variables y selecciona aquellas para las cuales la función regresa TRUE (por ejemplo: is.numeric() para seleccionar todas las variables numéricas)\n\n\n\nFunción rename()\nEsta función es una extensión de select(), dado que esta última permite cambiar el nombre de variables pero no es muy útil porque descarta todas las variables que no se mencionan explícitamente. En cambio rename() renombra variables mientras que mantiene las demás no mencionadas.\n\nPor ejemplo, cambiamos el nombre de la variable pob por población:\n\ndatos |&gt;\n  rename(\"población\" = pob)\n\n# A tibble: 48 × 4\n   jurisdiccion   año casos población\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 Buenos Aires  2015  1513  16626374\n 2 Buenos Aires  2016   957  16789474\n 3 CABA          2015   901   3054237\n 4 CABA          2016   427   3050000\n 5 Catamarca     2015    69    396552\n 6 Catamarca     2016    51    401575\n 7 Chaco         2015    15   1153846\n 8 Chaco         2016     9   1125000\n 9 Chubut        2015   110    567010\n10 Chubut        2016    89    577922\n# ℹ 38 more rows\n\n\n\n\n\nFunción filter()\nAsí como la función select() es utilizada para seleccionar columnas, la función filter() hace lo propio con las filas del conjunto de datos, produciendo un subconjunto de observaciones.\n\nVeamos un ejemplo sencillo sobre nuestros datos:\n\n\ndatos |&gt;\n  filter(jurisdiccion == \"Tucuman\")\n\n# A tibble: 2 × 4\n  jurisdiccion   año casos     pob\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Tucuman       2015   258 1592593\n2 Tucuman       2016   246 1618421\n\n\n\nUtiliza los mismos operadores de comparación propios del lenguaje R:\n\n\n\nOperador\nDescripción\n\n\n\n\n&lt;\nMenor a\n\n\n&gt;\nMayor a\n\n\n&lt;=\nMenor o igual a\n\n\n&gt;=\nMayor o igual a\n\n\n==\nIgual a\n\n\n!=\nNo igual a\n\n\n%in%\nEs parte de\n\n\nis.na()\nEs NA\n\n\n!is.na()\nNo es NA\n\n\n\n\n\n\nLo mismo con los operadores lógicos que se utilizan como conectores entre las expresiones:\n\n\n\nOperador\nDescripción\n\n\n\n\n&\nAND booleano\n\n\n|\nOR booleano\n\n\nxor()\nOR exclusivo\n\n\n!\nNOT\n\n\nany()\ncualquier TRUE\n\n\nall()\ntodos TRUE\n\n\n\n\nCuando usamos múltiples argumentos separados por coma dentro de filter() se combinan con un conector AND, es decir cada expresión debe ser verdadera para que una fila sea incluida en la salida.\nPor ejemplo:\n\nFiltramos a las observaciones que cumplan con la condición que casos sea mayor a 100 y población sea menor a 1000000:\n\ndatos |&gt;\n  filter(casos &gt; 100, pob &lt; 1000000)\n\n# A tibble: 7 × 4\n  jurisdiccion   año casos    pob\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Chubut        2015   110 567010\n2 Jujuy         2015   160 727273\n3 Jujuy         2016   133 734807\n4 Neuquen       2015   109 619318\n5 Neuquen       2016   101 627329\n6 Rio Negro     2015   112 700000\n7 Rio Negro     2016   105 709459\n\n\n\nPara combinaciones dentro de una misma variable debemos utilizar el conector OR (|) o más útil el operador %in%.\n\ndatos |&gt;\nfilter(jurisdiccion == “Buenos Aires” | jurisdiccion == “La Pampa”)\nFiltramos a las jurisdicciones “Buenos Aires” y “La Pampa”:\n\ndatos |&gt;\n  filter(jurisdiccion == \"Buenos Aires\" | jurisdiccion == \"La Pampa\")\n\n# A tibble: 4 × 4\n  jurisdiccion   año casos      pob\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 Buenos Aires  2015  1513 16626374\n2 Buenos Aires  2016   957 16789474\n3 La Pampa      2015    57   343373\n4 La Pampa      2016    67   345361\n\ndatos |&gt;\n  filter(jurisdiccion %in% c(\"Buenos Aires\", \"La Pampa\"))\n\n# A tibble: 4 × 4\n  jurisdiccion   año casos      pob\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 Buenos Aires  2015  1513 16626374\n2 Buenos Aires  2016   957 16789474\n3 La Pampa      2015    57   343373\n4 La Pampa      2016    67   345361\n\n\nFiltramos las observaciones de 2016 con casos mayores a 200 utilizando el conector AND (&). Es el mismo resultado que si utilizamos una coma:\n\ndatos |&gt;\n  filter(año == \"2016\" & casos &gt; 200)\n\n# A tibble: 6 × 4\n  jurisdiccion   año casos      pob\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 Buenos Aires  2016   957 16789474\n2 CABA          2016   427  3050000\n3 Cordoba       2016   368  3607843\n4 Mendoza       2016   254  1909774\n5 Salta         2016   230  1352941\n6 Tucuman       2016   246  1618421\n\n\nFiltramos las observaciones inversas a la anterior mediante xor(), que selecciona los valores de año y casos exclusivos (es decir que no se den ambos en TRUE):\n\ndatos |&gt; \n  filter(xor(año == \"2016\", casos &gt; 200))\n\n# A tibble: 25 × 4\n   jurisdiccion   año casos      pob\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 Buenos Aires  2015  1513 16626374\n 2 CABA          2015   901  3054237\n 3 Catamarca     2016    51   401575\n 4 Chaco         2016     9  1125000\n 5 Chubut        2016    89   577922\n 6 Cordoba       2015   468  3572519\n 7 Corrientes    2016    99  1076087\n 8 Entre Rios    2016   109  1329268\n 9 Formosa       2016    60   582524\n10 Jujuy         2016   133   734807\n# ℹ 15 more rows\n\n\n\n\n\nFunción arrange()\nLa función arrange() se utiliza para ordenar las filas de un conjunto de datos de acuerdo a una o varias columnas/variables. Por defecto, el ordenamiento es ascendente alfanumérico.\n\nOrdenamos la tabla datos por la variable pob (forma ascendente predeterminada):\n\ndatos |&gt;\n  arrange(pob)\n\n# A tibble: 48 × 4\n   jurisdiccion       año casos    pob\n   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Tierra del Fuego  2015    36 152542\n 2 Tierra del Fuego  2016    34 156682\n 3 Santa Cruz        2015    65 320197\n 4 Santa Cruz        2016    59 329609\n 5 La Pampa          2015    57 343373\n 6 La Pampa          2016    67 345361\n 7 La Rioja          2015    41 369369\n 8 La Rioja          2016     6 375000\n 9 Catamarca         2015    69 396552\n10 Catamarca         2016    51 401575\n# ℹ 38 more rows\n\n\nPara ordenar en forma descendente podemos utilizar desc() dentro de los argumentos de arrange():\n\ndatos |&gt;\n  arrange(desc(pob))\n\n# A tibble: 48 × 4\n   jurisdiccion   año casos      pob\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 Buenos Aires  2016   957 16789474\n 2 Buenos Aires  2015  1513 16626374\n 3 Cordoba       2016   368  3607843\n 4 Cordoba       2015   468  3572519\n 5 Santa Fe      2016   170  3400000\n 6 Santa Fe      2015   301  3382022\n 7 CABA          2015   901  3054237\n 8 CABA          2016   427  3050000\n 9 Mendoza       2016   254  1909774\n10 Mendoza       2015   316  1880952\n# ℹ 38 more rows\n\n\nPodemos combinar ordenamientos. Por ejemplo, en forma alfabética ascendente para jusrisdiccion y luego numérica descendente para casos:\n\ndatos |&gt;\n  arrange(jurisdiccion, desc(casos))\n\n# A tibble: 48 × 4\n   jurisdiccion   año casos      pob\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 Buenos Aires  2015  1513 16626374\n 2 Buenos Aires  2016   957 16789474\n 3 CABA          2015   901  3054237\n 4 CABA          2016   427  3050000\n 5 Catamarca     2015    69   396552\n 6 Catamarca     2016    51   401575\n 7 Chaco         2015    15  1153846\n 8 Chaco         2016     9  1125000\n 9 Chubut        2015   110   567010\n10 Chubut        2016    89   577922\n# ℹ 38 more rows\n\n\n\n\n\nFunción mutate()\nEsta función nos proporciona computar tranformaciones de variables en un conjunto de datos. A menudo, tendremos la necesidad de modificar variables existentes o crear nuevas variables que se calculan a partir de las que tenemos, mutate() nos ofrece una interfaz clara para realizar este tipo de operaciones.\n\nPor ejemplo, nos puede interesar calcular tasas crudas para cada jurisdicción y año, en función de los casos y el total de población:\n\ndatos |&gt;\n  mutate(tasa = casos/pob*100000)\n\n# A tibble: 48 × 5\n   jurisdiccion   año casos      pob  tasa\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 Buenos Aires  2015  1513 16626374  9.10\n 2 Buenos Aires  2016   957 16789474  5.70\n 3 CABA          2015   901  3054237 29.5 \n 4 CABA          2016   427  3050000 14   \n 5 Catamarca     2015    69   396552 17.4 \n 6 Catamarca     2016    51   401575 12.7 \n 7 Chaco         2015    15  1153846  1.30\n 8 Chaco         2016     9  1125000  0.8 \n 9 Chubut        2015   110   567010 19.4 \n10 Chubut        2016    89   577922 15.4 \n# ℹ 38 more rows\n\n\nObservemos que la función realiza el cálculo (en este caso tasas crudas por 100000 habitantes) e incorpora una nueva variable por cada observación con el resultado.\nTambién se pueden construir múltiples variables en la misma expresión, solamente separadas por comas:\n\ndatos |&gt;\n  mutate(tasaxcien_mil = casos/pob*100000, \n         tasaxdiez_mil = casos/pob*10000)\n\n# A tibble: 48 × 6\n   jurisdiccion   año casos      pob tasaxcien_mil tasaxdiez_mil\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 Buenos Aires  2015  1513 16626374          9.10         0.910\n 2 Buenos Aires  2016   957 16789474          5.70         0.570\n 3 CABA          2015   901  3054237         29.5          2.95 \n 4 CABA          2016   427  3050000         14            1.4  \n 5 Catamarca     2015    69   396552         17.4          1.74 \n 6 Catamarca     2016    51   401575         12.7          1.27 \n 7 Chaco         2015    15  1153846          1.30         0.130\n 8 Chaco         2016     9  1125000          0.8          0.08 \n 9 Chubut        2015   110   567010         19.4          1.94 \n10 Chubut        2016    89   577922         15.4          1.54 \n# ℹ 38 more rows\n\n\nSi necesitemos que estas dos nuevas variables queden dentro de la tabla de datos y no solo mostrarla en consola como hasta ahora, debemos utilizar el operador de asignación:\n\ndatos &lt;- datos |&gt;\n  mutate(tasaxcien_mil = casos/pob*100000, \n         tasaxdiez_mil = casos/pob*10000)\n\n\nLa propiedad imprescindible es que la función debe poder vectorizar: debe tomar un vector de valores como entrada, y devolver un vector con el mismo número de valores que la salida.\nNo hay forma de enumerar todas las funciones posibles que se podría usar, pero mencionaremos algunas que pueden ser útiles:\n\nOperadores aritméticos: +, -, *, /, ^.\nAritmética modular: %/% (división entera) y %% (resto), donde \\(x == y * (x \\ \\%/\\% \\ y) + (x\\ \\%\\% \\ y)\\). La aritmética modular es una herramienta útil porque te permite dividir números enteros en porciones.\nFunciones matemáticas: log(), log2(), log10(), exp(), sqrt(), abs(), etc.\nValores acumulados: R proporciona funciones para ejecutar sumas, productos, mínimos y máximos acumulados: cumsum(), cumprod(), cummin(), cummax(); y dplyr proporciona cummean() para promedios acumulados.\nClasificaciones (ranking): hay una serie de funciones de clasificación, por ejemplo min_rank(). Genera el tipo de clasificación habitual (1º, 2º, etc). El valor predeterminado relaciona los valores más pequeños a rangos pequeños; podemos usar desc(x) para invertir la relación (valores más grandes a rangos más pequeños).\n\nSi utilizamos el mismo nombre de una variable incluída dentro de la tabla de datos, estaremos sobrescribiendola (se usa cuando transformamos una variable, por ejemplo: le cambiamos su tipo de character a factor). Para que la variable sea nueva debe nombrarse con un nombre que no exista previamente dentro de la tabla de datos.\n\n\nFunción summarise()\nLa función summarise() (se puede escribir también summarize()) resume variables de un conjunto de datos.\n\ndatos |&gt;\n  summarise(promedio_casos = mean(casos), \n            casos_totales = sum(casos))\n\n# A tibble: 1 × 2\n  promedio_casos casos_totales\n           &lt;dbl&gt;         &lt;dbl&gt;\n1           192.          9211\n\n\nSu uso es muy interesante cuando la combinamos con group_by() (función que detallaremos luego). Esta situación permite estratificar los resultados por grupos específicos.\nPor ejemplo, podemos agrupar el por año y simultáneamente aplicar el mismo summarise() anterior:\n\ndatos |&gt; \n  group_by(año) |&gt; \n  summarise(promedio_casos = mean(casos), \n            casos_totales = sum(casos))\n\n# A tibble: 2 × 3\n    año promedio_casos casos_totales\n  &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1  2015           224.          5369\n2  2016           160.          3842\n\n\nEl resultado es una tabla con dos filas, una para cada grupo (año 2015 y año 2016) con los valores promedio y casos totales respectivos.\nAlgunas de las funciones del R base que se pueden utilizar dentro de los argumentos de esta función son:\n\nmin(): mínimo\nmax(): máximo\nmean(): media\nmedian(): mediana\nvar(): varianza\nsd(): desvío\nsum(): sumatoria\n\nOtras funciones que se pueden incorporar las provee el mismo paquete dplyr, por ejemplo:\n\nfirst(): primer valor en el vector\nlast(): último valor en el vector\nn(): número de valores en el vector\nn_distinct(): números de valores distintos en el vector\n\n\n\nFunción group_by()\nDecíamos recién que la función group_by() es útil cuando trabaja conjuntamente con summarise() dado que agrupa un conjunto de filas seleccionado en un conjunto de filas de resumen de acuerdo con los valores de una o más columnas o expresiones.\nPara ejemplificar su trabajo asociado obtendremos una nueva tabla con el cálculo de las tasas crudas para cada jurisdicción por año (similar al ejemplo de la aplicación de mutate():\n\ndatos |&gt;\n  group_by(jurisdiccion, año) |&gt; \n  summarise(tasa = casos/pob*100000)\n\n# A tibble: 48 × 3\n# Groups:   jurisdiccion [24]\n   jurisdiccion   año  tasa\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 Buenos Aires  2015  9.10\n 2 Buenos Aires  2016  5.70\n 3 CABA          2015 29.5 \n 4 CABA          2016 14   \n 5 Catamarca     2015 17.4 \n 6 Catamarca     2016 12.7 \n 7 Chaco         2015  1.30\n 8 Chaco         2016  0.8 \n 9 Chubut        2015 19.4 \n10 Chubut        2016 15.4 \n# ℹ 38 more rows\n\n\nEn la mayoría de estos ejemplos la salida es directa, es decir no construimos nuevos objetos contenedores de los datos producidos. Pero en muchas situaciones vamos a necesitar generar nuevos conjunto de datos con las transformaciones realizadas. Si en alguna de estas ocasiones llegamos a agrupar datos mediante group_by() y posteriormente necesitamos volver a tener la información desagrupada existe una función vinculada denominada ungroup().\n\n\nCombinaciones\nEn los ejemplos anteriores vimos como se van integrando alguna de las funciones mediante el uso de la tubería %&gt;% o |&gt;. La idea detrás de la búsqueda gramatical del paquete es poder enlazar las acciones para construir oraciones más complejas.\nUn ejemplo que podría integrar gran parte de los visto sería:\nObtener una nueva tabla con las tasas crudas de casos notificados de VIH, por año y jurisdicción, mayores a 20 x 100000 habitantes ordenadas de mayor a menor.\n\ndatos |&gt;                                  # siempre partimos de los datos\n  group_by(año, jurisdiccion) |&gt;          # agrupamos\n  summarise(tasa = casos/pob*100000) |&gt;   # resumimos\n  filter(tasa &gt; 20) |&gt;                    # filtramos\n  arrange(desc(tasa))                       # ordenamos   \n\n# A tibble: 5 × 3\n# Groups:   año [2]\n    año jurisdiccion      tasa\n  &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1  2015 CABA              29.5\n2  2015 Tierra del Fuego  23.6\n3  2015 Jujuy             22.0\n4  2016 Tierra del Fuego  21.7\n5  2015 Santa Cruz        20.3\n\n\nObservemos que una buena manera de construir el código es respetar un salto de línea para cada término de la oración para una lectura más clara.\nDemostramos así la potencialidad que tienen estas funciones combinadas donde en esta situación integramos las funciones group_by(), summarise() , filter() y arrange() en una misma operación.\n\n\nFunción count()\nEsta última función que presentamos permite contar rápidamente los valores únicos de una o más variables. Produce fácilmente tablas de frecuencias absolutas que luego posibilitan construir frecuencias relativas.\nLa aplicamos sobre la variable jurisdiccion de datos:\n\ndatos |&gt;\n  count(jurisdiccion)\n\n# A tibble: 24 × 2\n   jurisdiccion     n\n   &lt;chr&gt;        &lt;int&gt;\n 1 Buenos Aires     2\n 2 CABA             2\n 3 Catamarca        2\n 4 Chaco            2\n 5 Chubut           2\n 6 Cordoba          2\n 7 Corrientes       2\n 8 Entre Rios       2\n 9 Formosa          2\n10 Jujuy            2\n# ℹ 14 more rows\n\n\nTiene un par de argumentos opcionales:\n\nname: es el nombre de la columna con el conteo. Por defecto se llama n\nsort: ordena la tabla de frecuencia de mayor a menor\nwt: se puede opcionalmente incorporar una variable con la ponderación (factor de expansión) para el calculo de la frecuencia."
  },
  {
    "objectID": "tidyverse.html#gráficos-estadísticos-con-ggplot2",
    "href": "tidyverse.html#gráficos-estadísticos-con-ggplot2",
    "title": "Introducción a tidyverse",
    "section": "Gráficos estadísticos con ggplot2",
    "text": "Gráficos estadísticos con ggplot2\nggplot2 es un paquete que se autodefine como librería para “crear elegantes visualizaciones de datos usando una gramática de gráficos”\nPropone una forma intuitiva de construir gráficos basada en The Grammar of Graphics, a partir de utilizar capas y un sistema apoyado en tres componentes básicos:\n\ndatos\ncoordenadas\nobjetos geométricos\n\nLa estructura para construir un gráfico es la siguiente:\n\n\n\n\n\n\nAnatomía de gráficos con ggplot2\nEl paquete se basa en una gramática de gráficos que puede ser entendida a partir de conocer sus componentes:\n\n\n\n\n\n\ndata: es aquél conjunto de datos que vamos a graficar, con toda la información pertinente para realizar el gráfico.\naes(): reducción de aesthetic mapping o mapeo estético en el que se puede declarar todo lo que puede ser visible de un gráfico.\ngeoms: son representaciones para dibujar gráficos (puntos, líneas, cajas, entre otros).\nstats: son aquellas transformaciones estadísticas que le hacemos a los datos. Nos ayudan a hacer un resumen del conjunto de datos para visualizar mejor (por ejemplo, la media o la mediana como estadísticas de tendencia central).\nscales: facilitan colorear (o escalar) los datos según distintas variables. Dibujan los ejes y las leyendas.\ncoordinate systems: es el sistema de coordenadas para el mapeo del gráfico en un plano bidimensional.\nfacets: nos permiten partir el conjunto de datos según factores para graficar en viñetas separadas creando matrices gráficas.\nthemes: son conjuntos de características gráficas que permiten controlar la apariencia general de todos los elementos que no son datos (por ejemplo, el color del fondo o el tipo de fuente).\n\nAntes de comenzar a explicar cada componente vamos a leer un conjunto de datos que nos permita mostrar los ejemplos gráficos.\n\nlibrary(tidyverse)\n\nfacultad &lt;- read_csv(\"datos/facultad.csv\") # lectura\n\nhead(facultad) # mostramos las 6 primeras observaciones\n\n# A tibble: 6 × 18\n     hc sexo   edad ant_diabetes ant_tbc ant_cancer ant_obesidad ant_ecv ant_ht\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt; \n1 26880 M        17 NO           NO      NO         SI           NO      SI    \n2 26775 M        18 SI           NO      NO         NO           NO      NO    \n3 26877 M        18 SI           NO      SI         NO           NO      SI    \n4 26776 M        18 NO           NO      NO         SI           SI      NO    \n5 26718 M        18 NO           NO      NO         NO           NO      SI    \n6 26738 M        18 NO           NO      NO         NO           NO      SI    \n# ℹ 9 more variables: ant_col &lt;chr&gt;, fuma &lt;chr&gt;, edadini &lt;dbl&gt;, cantidad &lt;dbl&gt;,\n#   col &lt;dbl&gt;, peso &lt;dbl&gt;, talla &lt;dbl&gt;, sist &lt;dbl&gt;, diast &lt;dbl&gt;\n\n\nEl archivo leído se llama “facultad.csv” y contiene información de salud sobre ingresantes a una facultad tales como sexo, edad, talla y peso, entre otras. (son datos ficticios con fines docentes).\n\n\nMapeo estético (aesthetic mapping) y objetos geométricos (geom)\nDecíamos que aes() hace referencia al contenido estético del gráfico. Es decir, la función le brinda indicios a ggplot2 sobre cómo dibujar los distintas líneas, formas, colores y tamaños.\nEs importante notar que aes() crea una nueva capa en relación a las variables y agrega leyendas a los gráficos. Al incorporar aes() al llamado de ggplot() estamos compartiendo la información estética en todas las capas. Si deseamos que esa información sólo esté en una de las capas, debemos usar aes() en la capa correspondiente.\nVeamos como funciona y sus diferencias:\n\nfacultad |&gt;\n  ggplot(aes(talla, peso)) # solo la capa estética aes()\n\n\n\n\n\n\n\n\nEste gráfico solo contiene los ejes que especificamos (peso y talla) pero no contiene los datos. Si deseamos incorporarlos agregamos una capa de puntos con geom_point() a través del símbolo +:\n\nfacultad |&gt;\n  ggplot(aes(talla, peso))  + \n  geom_point()                    # agregamos la capa geométrica de puntos\n\n\n\n\n\n\n\n\nPodemos diferenciar los puntos según se traten de ingresantes mujeres y hombres, asociando el argumento color dentro de aes() con la variable sexo:\n\nfacultad |&gt;\n  ggplot(aes(talla, peso, color = sexo)) +\n  geom_point()\n\n\n\n\n\n\n\n\nEstos gráficos también posibilitan el agregado de otra capa geométrica, por ejemplo rectas de regresión para cada grupo según sexo:\n\nfacultad |&gt;\n  ggplot(aes(talla, peso, color = sexo)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")            # agregamos una segunda capa geométrica \n\n\n\n\n\n\n\n\nEsta función geom_smooth() posee distintos métodos y en este ejemplo utilizamos el de regresión lineal entre talla y peso junto a sus intervalos de confianza.\nA continuación vamos a ver que diferencias existen cuando aes() se encuentra dentro del ggplot() y cuando se ubica en otras capas de funciones como en geom_point().\nDecíamos anteriormente que al incorporar aes() al llamado de ggplot() estamos compartiendo la información estética en todas las capas.\nEntonces si quitamos aes() de allí y lo ubicamos en una capa única, esta configuración deja de afectar al conjunto del gráfico:\n\nfacultad |&gt;\n  ggplot(aes(talla, peso)) + \n  geom_point(aes(color = sexo)) + # color esta definido en el aes() \n                                  # de la capa geométrica\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nEn este ejemplo, aes() para el color solo se ubica dentro de geom_point() y por lo tanto dibuja los puntos con sus respectivos colores, pero no afecta a la capa de geom_smooth() produciendo solo una línea de regresión para el conjunto de puntos.\nEs decir que geom_smooth() no recibe la orden de agrupar según sexo, a raíz de no haber definido color dentro del aes() general.\nEste comportamiento nos permite gran versatilidad en los gráficos que realicemos.\nAlgunas otras funciones de geom_ son:\n\ngeom_line(): para líneas.\ngeom_boxplot(): para boxplot.\ngeom_histogram(): para histogramas.\ngeom_density(): para curvas de densidad.\ngeom_bar(): para barras.\n\nEstas funciones geométricas aplicadas sobre los mismos datos definen el tipo de gráfico.\nPara ejemplificar, podemos gráficar dos variables como sexo y talla generando una base a la que sumaremos capas diferentes de geom():\n\n# Gráfico de puntos\nfacultad |&gt;\n  ggplot(aes(sexo, talla, color = sexo)) + \n  geom_point()                # capa geométrica de puntos\n\n\n\n\n\n\n\n# Boxplot\nfacultad |&gt;\n  ggplot(aes(sexo, talla, color = sexo)) + \n  geom_boxplot()            # capa geométrica de boxplot\n\n\n\n\n\n\n\n# Entramado de puntos\nfacultad |&gt;\n  ggplot(aes(sexo, talla, color = sexo)) + \n  geom_jitter()               # capa geométrica jitter (entramado de puntos)\n\n\n\n\n\n\n\n# Gráfico de violín\nfacultad |&gt;\n  ggplot(aes(sexo, talla, fill = sexo)) + \n  geom_violin()            # capa geométrica de violin\n\n\n\n\n\n\n\n\nObservemos que en este último gráfico cambiamos, dentro de aes(), color por fill. Mientras que color define el color del contorno del polígono, la línea de una recta o curva y los puntos, fill define el relleno de los objetos como es el caso de los violines construidos o cualquier elemento geométrico de polígono.\n\n\nEscalas (scale)\nLas configuraciones que se pueden realizar con scale son numerosas. Entre ellas encontramos cambios de color de contorno y relleno, cambios de posición, de tamaño y tipo de línea.\nEl argumento para modificar valores de escala comienzan siempre con con scale_ (por ejemplo scale_fill_ ).\nSigamos trabajando con el conjunto de datos leído para mostrar ejemplos de gráficos donde agregamos capas de escala para color de relleno:\n\nfacultad |&gt;\n  ggplot(aes(sexo, talla, fill = sexo)) + \n  geom_boxplot() +\n  scale_fill_brewer(palette = \"Oranges\")   # paleta de los naranjas\n\n\n\n\n\n\n\n\nEn este ejemplo aplicamos una capa scale_fill_brewer() con una paleta de colores (Oranges) que se vincula con el argumento fill de aes() y definen los colores del boxplot.\nLo mismo hacemos para una gama de grises mediante scale_fill_grey():\n\nfacultad |&gt;\n  ggplot(aes(sexo, talla, fill = sexo)) + \n  geom_boxplot() +\n  scale_fill_grey(start = 0.4, end = 0.8)   # paleta de los grises\n\n\n\n\n\n\n\n\nOtro uso de escalas, esta vez aplicado a los ejes, es la inversión del eje X:\n\nfacultad |&gt;\n  ggplot(aes(talla, peso, color = sexo)) +\n  geom_point() +\n  scale_x_reverse()     # escala inversa de x\n\n\n\n\n\n\n\n\nComo se observa en el gráfico la inclusión de scale_x_reverse() provoca que la escala X se invierta, quedando la talla ordenada de mayor a menor.\nPor último, otro ejemplo interesante es aplicado a escalas de etiquetado de ejes. Volvamos al ejemplo reciente de boxplot con relleno en escala de grises, su eje Y se dibuja predeterminado desde 130 a casi 200 cms con cortes cada 5 cms y etiquetas cada 10 cms.\nCon escalas continuas manuales de la forma scale_*_continuos() podemos personalizar el eje Y:\n\nfacultad |&gt;\n  ggplot(aes(sexo, talla, fill = sexo)) + \n  geom_boxplot() +\n  scale_fill_grey(start = 0.4, end = 0.8)   +\n  scale_y_continuous(breaks = seq(130,200,2))\n\n\n\n\n\n\n\n\nEn este caso particular definimos un eje Y con etiquetas de 2 en 2, mediante la línea scale_y_continuous(breaks = seq(130,200,2)).\n\n\nTransformaciones estadísticas (stat)\nAlgunos gráficos no requieren de transformaciones estadísticas, en cambio, otros como boxplot, histogramas, etc. poseen valores predeterminados de stat que pueden ser modificados.\nEstos valores se encuentra en forma de argumentos dentro de la función geométrica, por ejemplo para los histograma el argumento bins define la cantidad de intervalos de clase:\n\nfacultad |&gt;\n  ggplot(aes(edad)) +\n    geom_histogram(bins = nclass.Sturges(facultad$edad), fill = \"Blue\")\n\n\n\n\n\n\n\n\nVemos que el gráfico se construyó utilizando la regla de Sturges para determinar la cantidad de intervalos de clase para la variable edad. (función nclass.Sturges())\nOtras transformaciones estadísticas se incorporan como capas independientes, por ejemplo si queremos agregar los valores de media a los boxplot de talla según sexo construidos anteriormente:\n\nfacultad |&gt;\n  ggplot(aes(sexo, talla, fill = sexo)) + \n  geom_boxplot() +\n  scale_fill_brewer(palette = \"Greens\") +\n  stat_summary(fun = mean, color = \"darkred\", geom = \"point\", \n               shape = 18, size = 3)\n\n\n\n\n\n\n\n\nAquí la capa completa surge a partir de la función stat_summary(), con argumentos que indican que se aplique la función mean. Incluye también la definición del objeto geométrico (point) que representa el valor de media (color, forma y tamaño)\n\n\nFacetado (facet)\nCon facet es posible separar gráficos en distintas ventanas o viñetas, creando matrices de gráficos separados por grupos de datos, a partir de la estratificación, en función de diferentes categorías de una variable cualitativa.\nEste comportamiento es sumamente útil cuando tenemos más de una variable categórica o cuando deseamos utilizar color para simbolizar otra variable.\nggplot ofrece dos posibilidades de hacer el facetado:\n\nfacet_wrap() – define subgrupos a partir de los niveles de una sola variable categórica\nfacet_grid() – define subgrupos a partir del cruce de dos variables de categóricas.\n\nUna aplicación de facet_wrap() podría ser que el primer gráfico que hicimos de dispersión de puntos con las variables talla y peso se visualice en dos gráficos distintos según cada categoría de sexo:\n\nfacultad |&gt;\n  ggplot(aes(talla, peso, color = sexo)) +\n     geom_point() +\n     facet_wrap(~sexo)\n\n\n\n\n\n\n\n\nUsamos facet_grid() para crear una matriz producto del cruce de las variables fuma y sexo. Dentro de la cuadrícula graficaremos histogramas de la variable peso coloreados por sexo:\n\nfacultad |&gt;\n  ggplot(aes(peso, fill = sexo)) +\n     geom_histogram(bins = nclass.Sturges(facultad$peso)) +\n     scale_fill_brewer(palette = \"Set1\") +\n     facet_grid(sexo ~ fuma)\n\n\n\n\n\n\n\n\nSi observamos las 4 líneas que integran todas las capas del código de ggplot2 notaremos que estamos integrando varias de las funciones que fuimos mostrando.\nSe hace imposible generar todas combinaciones posibles dada la variedad y extensión de argumentos que posee el paquete. De todas formas, el objetivo de este material es entender la base de funcionamiento, es decir la estructura “gramatical” que proponen sus autores.\n\n\nSistema de coordenadas (Coordinate Systems)\nEn algunas ocasiones puede que necesitemos introducir modificaciones en el sistema de coordinadas predeterminado.\nSobre las coordenadas cartesianas iniciales se puede invertir la orientación para que, por ejemplo, las barras se dibujen horizontales:\n\nfacultad |&gt;\n  ggplot(aes(sexo, fill = sexo)) +\n     scale_fill_brewer(palette = \"Set2\") +\n     geom_bar() +\n     coord_flip()   # invierte disposición de ejes\n\n\n\n\n\n\n\n\n\n\nTemas (themes)\nEl paquete ofrece un conjunto reducido de temas gráficos. El tema por defecto o inicial es theme_gray() pero se puede modificar a partir de agregar una capa de tema dentro de la estructura ggplot.\nA modo de ejemplo repetimos el último gráfico con el tema blanco y negro ( theme_bw() ):\n\nfacultad |&gt;\n  ggplot(aes(sexo, fill = sexo)) +\n     scale_fill_brewer(palette = \"Set2\") +\n     geom_bar() +\n     coord_flip() +\n     theme_bw()  # tema blanco y negro\n\n\n\n\n\n\n\n\nOtro tema que podemos utilizar es theme_dark() que tiene un fondo gris oscuro:\n\nfacultad |&gt;\n  ggplot(aes(sexo, fill = sexo)) +\n     scale_fill_brewer(palette = \"Set2\") +\n     geom_bar() +\n     coord_flip() +\n     theme_dark()\n\n\n\n\n\n\n\n\nEl siguiente cuadro muestra el nombre y presentación de los temas que contiene el paquete:\n\n\n\n\n\nContinuando con cuestiones estéticas en otra capa se pueden definir etiquetas, como título, subtítulo y nombres de ejes.\nLa forma de la función con argumentos básicos es labs( x = “Etiqueta X\", y = “Etiqueta Y\", title =“Título del gráfico\", subtitle = \"Subtítulo del gráfico\").\nAdemás se utiliza la función theme() para configurar el tipo de fuente y tamaño, entre otras opciones posibles:\n\nfacultad |&gt;\n  ggplot(aes(sexo, fill = sexo)) +\n     scale_fill_brewer(palette = \"Set2\") +\n     geom_bar() +\n     coord_flip() +\n     labs(y = \"Cantidad\", title = \"Distribución de sexo\") +\n     theme(plot.title=element_text(face=\"italic\", size=16)) \n\n\n\n\n\n\n\n\n\n\nPaquete esquisse\nEsquisse es un paquete que contiene una aplicación asistente para crear gráficos ggplot2 de forma interactiva. Basta con arrastrar y soltar las variables para asignarlas a diferentes estéticas.\nPodemos visualizar rápidamente los datos de acuerdo con su tipo, exportarlos en varios formatos y recuperar el código para reproducir el gráfico.\nEl paquete se instala mediante el menú Packages de RStudio o ejecutando:\n\ninstall.packages(\"esquisse\")\n\nLuego se puede acceder a la aplicación por medio del acceso Addins:\n\n\n\n\n\no ejecutando en consola:\n\nesquisser()\n\nTambién se puede agregar el nombre de la tabla de datos dentro de los paréntesis\n\nesquisser(datos)\n\nPara más información consultar en la viñeta del paquete en CRAN."
  },
  {
    "objectID": "unidad_1.html",
    "href": "unidad_1.html",
    "title": "Unidad 1: Revisión Sistemática",
    "section": "",
    "text": "Este material es parte del curso interno de Introducción a la Revisión Sistemática con Meta-análisis  © 2025 Instituto Nacional de Epidemiología “Dr. Juan H. Jara” (ANLIS) -  CC BY-NC 4.0      \n\n\n Volver arriba"
  }
]